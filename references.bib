
@book{adlerRandomFieldsGeometry2007,
  title = {Random {{Fields}} and {{Geometry}}},
  author = {Adler, Robert J. and Taylor, Jonathan E.},
  date = {2007},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  issn = {1439-7382},
  doi = {10.1007/978-0-387-48116-6},
  isbn = {978-0-387-48112-8},
  langid = {english},
  keywords = {Geometry,Mathematical Methods in Physics,Mathematics,Mathematics and Statistics,Probability Theory and Stochastic Processes,Statistics; general},
  file = {/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Random Fields and Geometry.pdf}
}

@book{azaisLevelSetsExtrema2009,
  title = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  author = {Azais, Jean-Marc and Wschebor, Mario},
  date = {2009-02-17},
  eprint = {vF36BsG32CEC},
  eprinttype = {googlebooks},
  publisher = {{John Wiley \& Sons}},
  abstract = {A timely and comprehensive treatment of random field theory with applications across diverse areas of study Level Sets and Extrema of Random Processes and Fields discusses how to understand the properties of the level sets of paths as well as how to compute the probability distribution of its extremal values, which are two general classes of problems that arise in the study of random processes and fields and in related applications. This book provides a unified and accessible approach to these two topics and their relationship to classical theory and Gaussian processes and fields, and the most modern research findings are also discussed. The authors begin with an introduction to the basic concepts of stochastic processes, including a modern review of Gaussian fields and their classical inequalities. Subsequent chapters are devoted to Rice formulas, regularity properties, and recent results on the tails of the distribution of the maximum. Finally, applications of random fields to various areas of mathematics are provided, specifically to systems of random equations and condition numbers of random matrices. Throughout the book, applications are illustrated from various areas of study such as statistics, genomics, and oceanography while other results are relevant to econometrics, engineering, and mathematical physics. The presented material is reinforced by end-of-chapter exercises that range in varying degrees of difficulty. Most fundamental topics are addressed in the book, and an extensive, up-to-date bibliography directs readers to existing literature for further study. Level Sets and Extrema of Random Processes and Fields is an excellent book for courses on probability theory, spatial statistics, Gaussian fields, and probabilistic methods in real computation at the upper-undergraduate and graduate levels. It is also a valuable reference for professionals in mathematics and applied fields such as statistics, engineering, econometrics, mathematical physics, and biology.},
  isbn = {978-0-470-43463-5},
  langid = {english},
  pagetotal = {407},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  file = {/Users/felix/paper/2009_Azais_Wschebor/Azais_Wschebor_2009_Level Sets and Extrema of Random Processes and Fields.pdf}
}

@article{bahriStatisticalMechanicsDeep2020,
  title = {Statistical {{Mechanics}} of {{Deep Learning}}},
  author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
  date = {2020},
  journaltitle = {Annual Review of Condensed Matter Physics},
  volume = {11},
  number = {1},
  pages = {501--528},
  doi = {10.1146/annurev-conmatphys-031119-050745},
  abstract = {The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.},
  keywords = {chaos,dynamical phase transitions,interacting particle systems,jamming,machine learning,neural networks,nonequilibrium statistical mechanics,random matrix theory,spin glasses},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-conmatphys-031119-050745},
  file = {/Users/felix/paper/2020_Bahri et al/Bahri et al_2020_Statistical Mechanics of Deep Learning.pdf}
}

@article{brayStatisticsCriticalPoints2007,
  title = {The Statistics of Critical Points of {{Gaussian}} Fields on Large-Dimensional Spaces},
  author = {Bray, Alan J. and Dean, David S.},
  date = {2007-04-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {98},
  number = {15},
  eprint = {cond-mat/0611023},
  eprinttype = {arxiv},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.98.150201},
  abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems, and to landscape scenarios coming from the anthropic approach to string theory.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces2.pdf;/Users/felix/Zotero/storage/RV2A6TST/0611023.html}
}

@article{deanExtremeValueStatistics2008,
  title = {Extreme Value Statistics of Eigenvalues of {{Gaussian}} Random Matrices},
  author = {Dean, David S. and Majumdar, Satya N.},
  date = {2008-04-10},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {77},
  number = {4},
  pages = {041108},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.77.041108},
  abstract = {We compute exact asymptotic results for the probability of the occurrence of large deviations of the largest (smallest) eigenvalue of random matrices belonging to the Gaussian orthogonal, unitary, and symplectic ensembles. In particular, we show that the probability that all the eigenvalues of an (N√óN) random matrix are positive (negative) decreases for large N as ‚àºexp [‚àíŒ≤Œ∏(0)N2] where the Dyson index Œ≤ characterizes the ensemble and the exponent Œ∏(0)=(ln 3)/4=0.274653‚Ä¶ is universal. We compute the probability that the eigenvalues lie in the interval [Œ∂1,Œ∂2] which allows us to calculate the joint probability distribution of the minimum and the maximum eigenvalue. As a by-product, we also obtain exactly the average density of states in Gaussian ensembles whose eigenvalues are restricted to lie in the interval [Œ∂1,Œ∂2], thus generalizing the celebrated Wigner semi-circle law to these restricted ensembles. It is found that the density of states generically exhibits an inverse square-root singularity at the location of the barriers. These results are confirmed by numerical simulations. Some of the results presented in detail here were announced in a previous paper [D. S. Dean and S. N. Majumdar, Phys. Rev. Lett. 97, 160201 (2006)].},
  file = {/Users/felix/paper/2008_Dean_Majumdar/Dean_Majumdar_2008_Extreme value statistics of eigenvalues of Gaussian random matrices.pdf;/Users/felix/Zotero/storage/RNPNPX7T/PhysRevE.77.html}
}

@unpublished{fyodorovHighDimensionalRandomFields2013,
  title = {High-{{Dimensional Random Fields}} and {{Random Matrix Theory}}},
  author = {Fyodorov, Yan V.},
  date = {2013-11-18},
  eprint = {1307.2379},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:math-ph},
  url = {http://arxiv.org/abs/1307.2379},
  urldate = {2022-04-12},
  abstract = {Our goal is to discuss in detail the calculation of the mean number of stationary points and minima for random isotropic Gaussian fields on a sphere as well as for stationary Gaussian random fields in a background parabolic confinement. After developing the general formalism based on the high-dimensional Kac-Rice formulae we combine it with the Random Matrix Theory (RMT) techniques to perform analysis of the random energy landscape of \$p-\$spin spherical spinglasses and a related glass model, both displaying a zero-temperature one-step replica symmetry breaking glass transition as a function of control parameters (e.g. a magnetic field or curvature of the confining potential). A particular emphasis of the presented analysis is on understanding in detail the picture of "topology trivialization" (in the sense of drastic reduction of the number of stationary points) of the landscape which takes place in the vicinity of the zero-temperature glass transition in both models. We will reveal the important role of the GOE "edge scaling" spectral region and the Tracy-Widom distribution of the maximal eigenvalue of GOE matrices for providing an accurate quantitative description of the universal features of the topology trivialization scenario.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2013_Fyodorov/Fyodorov_2013_High-Dimensional Random Fields and Random Matrix Theory.pdf;/Users/felix/Zotero/storage/CWGQ4C9V/1307.html}
}

@book{hirschDifferentialTopology1976,
  title = {Differential {{Topology}}},
  author = {Hirsch, Morris W.},
  date = {1976},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {33},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4684-9449-5},
  isbn = {978-1-4684-9451-8 978-1-4684-9449-5},
  keywords = {differential topology,Differentialtopologie,Immersion,manifold,topology},
  file = {/Users/felix/paper/1976_Hirsch/Hirsch_1976_Differential Topology.pdf}
}

@article{mezardReplicaFieldTheory1991,
  title = {Replica Field Theory for Random Manifolds},
  author = {M√©zard, Marc and Parisi, Giorgio},
  date = {1991-06-01},
  journaltitle = {Journal de Physique I},
  shortjournal = {J. Phys. I France},
  volume = {1},
  number = {6},
  pages = {809--836},
  publisher = {{EDP Sciences}},
  issn = {1155-4304, 1286-4862},
  doi = {10.1051/jp1:1991171},
  abstract = {Journal de Physique I, Journal de Physique Archives repr√©sente une mine d informations facile √† consulter sur la mani√®re dont la physique a √©t√© publi√©e depuis 1872.},
  langid = {english},
  file = {/Users/felix/paper/1991_M√©zard_Parisi/M√©zard_Parisi_1991_Replica field theory for random manifolds.pdf;/Users/felix/Zotero/storage/5NEYZNW7/jp1v1p809.html}
}

@incollection{mezardSpinGlassesIntroduction1994,
  title = {Spin {{Glasses}}: {{An Introduction}}},
  shorttitle = {Spin {{Glasses}}},
  booktitle = {From {{Statistical Physics}} to {{Statistical Inference}} and {{Back}}},
  author = {M√©zard, Marc},
  editor = {Grassberger, Peter and Nadal, Jean-Pierre},
  date = {1994},
  series = {{{NATO ASI Series}}},
  pages = {183--193},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-011-1068-6_11},
  abstract = {This is a short introduction to spin glass theory for non physicists. It is not at all a review paper. More detailed presentations can be found in the references ([1]),([2]),([3]), and an excellent introduction to random systems for non specialists is ([4]). I also refer to these reviews for mentioning the original works and their references, which are systematically avoided here.},
  isbn = {978-94-011-1068-6},
  langid = {english},
  keywords = {Free Energy Density,Pure State,Spin Glass,Symmetry Breaking,Ultrametric Space}
}

@inproceedings{penningtonGeometryNeuralNetwork2017,
  title = {Geometry of {{Neural Network Loss Surfaces}} via {{Random Matrix Theory}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pennington, Jeffrey and Bahri, Yasaman},
  date = {2017-07-17},
  pages = {2798--2806},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/pennington17a.html},
  urldate = {2022-04-12},
  abstract = {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, ùúôœï\textbackslash phi, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1‚àíùúô)21/2(1‚àíœï)21/2(1-\textbackslash phi)\^2.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2017_Pennington_Bahri/Pennington_Bahri_2017_Geometry of Neural Network Loss Surfaces via Random Matrix Theory.pdf;/Users/felix/Zotero/storage/BV2L2GUY/Pennington and Bahri - 2017 - Geometry of Neural Network Loss Surfaces via Rando.pdf}
}

@article{scheuererCovarianceModelsDivergenceFree2012,
  title = {Covariance {{Models}} for {{Divergence-Free}} and {{Curl-Free Random Vector Fields}}},
  author = {Scheuerer, Michael and Schlather, Martin},
  date = {2012-07-01},
  journaltitle = {Stochastic Models},
  volume = {28},
  number = {3},
  pages = {433--451},
  publisher = {{Taylor \& Francis}},
  issn = {1532-6349},
  doi = {10.1080/15326349.2012.699756},
  abstract = {We construct matrix-valued covariance functions and in ‚Ñù2 and ‚Ñù3, starting from an arbitrary scalar-valued variogram. It is shown that sufficiently smooth random vector fields (RVFs) with these covariance functions have divergence-free and curl-free sample paths, respectively. Conversely, essentially all models with such sample paths can be obtained via our construction. Extensions to space-time RVFs are possible. RVFs with divergence-free and curl-free sample paths can be utilised in meteorological applications e.g. for modelling and interpolating wind fields.},
  keywords = {Curl-free,Divergence-free,Matrix-valued covariance function,Primary 60G60,Sample path properties,Secondary 60G17; 62H11; 86A32,Vector-valued random field},
  annotation = {\_eprint: https://doi.org/10.1080/15326349.2012.699756},
  file = {/Users/felix/Zotero/storage/KDSCGVKT/15326349.2012.html}
}

@unpublished{swirszczLocalMinimaTraining2017a,
  title = {Local Minima in Training of Neural Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2017-02-17},
  eprint = {1611.06310},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.06310},
  urldate = {2022-04-12},
  abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/paper/2017_Swirszcz et al/Swirszcz et al_2017_Local minima in training of neural networks2.pdf;/Users/felix/Zotero/storage/BSW5BT3W/1611.html}
}


