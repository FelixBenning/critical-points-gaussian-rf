@book{adlerRandomFieldsGeometry2007,
  title = {Random {{Fields}} and {{Geometry}}},
  author = {Adler, Robert J. and Taylor, Jonathan E.},
  date = {2007},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  issn = {1439-7382},
  doi = {10.1007/978-0-387-48116-6},
  isbn = {978-0-387-48112-8},
  langid = {english},
  keywords = {Geometry,Mathematical Methods in Physics,Mathematics,Mathematics and Statistics,Probability Theory and Stochastic Processes,Statistics; general},
  file = {/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Random Fields and Geometry.pdf}
}

@book{azaisLevelSetsExtrema2009,
  title = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  author = {Azais, Jean-Marc and Wschebor, Mario},
  date = {2009-02-17},
  eprint = {vF36BsG32CEC},
  eprinttype = {googlebooks},
  publisher = {{John Wiley \& Sons}},
  abstract = {A timely and comprehensive treatment of random field theory with applications across diverse areas of study Level Sets and Extrema of Random Processes and Fields discusses how to understand the properties of the level sets of paths as well as how to compute the probability distribution of its extremal values, which are two general classes of problems that arise in the study of random processes and fields and in related applications. This book provides a unified and accessible approach to these two topics and their relationship to classical theory and Gaussian processes and fields, and the most modern research findings are also discussed. The authors begin with an introduction to the basic concepts of stochastic processes, including a modern review of Gaussian fields and their classical inequalities. Subsequent chapters are devoted to Rice formulas, regularity properties, and recent results on the tails of the distribution of the maximum. Finally, applications of random fields to various areas of mathematics are provided, specifically to systems of random equations and condition numbers of random matrices. Throughout the book, applications are illustrated from various areas of study such as statistics, genomics, and oceanography while other results are relevant to econometrics, engineering, and mathematical physics. The presented material is reinforced by end-of-chapter exercises that range in varying degrees of difficulty. Most fundamental topics are addressed in the book, and an extensive, up-to-date bibliography directs readers to existing literature for further study. Level Sets and Extrema of Random Processes and Fields is an excellent book for courses on probability theory, spatial statistics, Gaussian fields, and probabilistic methods in real computation at the upper-undergraduate and graduate levels. It is also a valuable reference for professionals in mathematics and applied fields such as statistics, engineering, econometrics, mathematical physics, and biology.},
  isbn = {978-0-470-43463-5},
  langid = {english},
  pagetotal = {407},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  file = {/Users/felix/paper/2009_Azais_Wschebor/Azais_Wschebor_2009_Level Sets and Extrema of Random Processes and Fields.pdf}
}

@article{bahriStatisticalMechanicsDeep2020,
  title = {Statistical {{Mechanics}} of {{Deep Learning}}},
  author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
  date = {2020},
  journaltitle = {Annual Review of Condensed Matter Physics},
  volume = {11},
  number = {1},
  pages = {501--528},
  doi = {10.1146/annurev-conmatphys-031119-050745},
  abstract = {The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.},
  keywords = {chaos,dynamical phase transitions,interacting particle systems,jamming,machine learning,neural networks,nonequilibrium statistical mechanics,random matrix theory,spin glasses},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-conmatphys-031119-050745},
  file = {/Users/felix/paper/2020_Bahri et al/Bahri et al_2020_Statistical Mechanics of Deep Learning.pdf}
}

@article{baldiNeuralNetworksPrincipal1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  shorttitle = {Neural Networks and Principal Component Analysis},
  author = {Baldi, Pierre and Hornik, Kurt},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {1},
  pages = {53--58},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90014-2},
  abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  langid = {english},
  keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
  file = {/Users/felix/paper/1989_Baldi_Hornik/Baldi_Hornik_1989_Neural networks and principal component analysis.pdf;/Users/felix/Zotero/storage/PAYJZGVA/0893608089900142.html}
}

@article{brayStatisticsCriticalPoints2007,
  title = {The Statistics of Critical Points of {{Gaussian}} Fields on Large-Dimensional Spaces},
  author = {Bray, Alan J. and Dean, David S.},
  date = {2007-04-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {98},
  number = {15},
  eprint = {cond-mat/0611023},
  eprinttype = {arxiv},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.98.150201},
  abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems, and to landscape scenarios coming from the anthropic approach to string theory.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces2.pdf;/Users/felix/Zotero/storage/RV2A6TST/0611023.html}
}

@online{chengDifferentiationIntegralSign2013,
  title = {Differentiation under the Integral Sign},
  author = {Cheng, Steve},
  date = {2013-03-22},
  url = {https://planetmath.org/differentiationundertheintegralsign},
  urldate = {2022-12-20},
  annotation = {username: stevecheng (10074)},
  file = {/Users/felix/Zotero/storage/24I7IQN7/differentiationundertheintegralsign.html}
}

@article{deanExtremeValueStatistics2008,
  title = {Extreme Value Statistics of Eigenvalues of {{Gaussian}} Random Matrices},
  author = {Dean, David S. and Majumdar, Satya N.},
  date = {2008-04-10},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {77},
  number = {4},
  pages = {041108},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.77.041108},
  abstract = {We compute exact asymptotic results for the probability of the occurrence of large deviations of the largest (smallest) eigenvalue of random matrices belonging to the Gaussian orthogonal, unitary, and symplectic ensembles. In particular, we show that the probability that all the eigenvalues of an (N×N) random matrix are positive (negative) decreases for large N as ∼exp [−βθ(0)N2] where the Dyson index β characterizes the ensemble and the exponent θ(0)=(ln 3)/4=0.274653… is universal. We compute the probability that the eigenvalues lie in the interval [ζ1,ζ2] which allows us to calculate the joint probability distribution of the minimum and the maximum eigenvalue. As a by-product, we also obtain exactly the average density of states in Gaussian ensembles whose eigenvalues are restricted to lie in the interval [ζ1,ζ2], thus generalizing the celebrated Wigner semi-circle law to these restricted ensembles. It is found that the density of states generically exhibits an inverse square-root singularity at the location of the barriers. These results are confirmed by numerical simulations. Some of the results presented in detail here were announced in a previous paper [D. S. Dean and S. N. Majumdar, Phys. Rev. Lett. 97, 160201 (2006)].},
  file = {/Users/felix/paper/2008_Dean_Majumdar/Dean_Majumdar_2008_Extreme value statistics of eigenvalues of Gaussian random matrices.pdf;/Users/felix/Zotero/storage/RNPNPX7T/PhysRevE.77.html}
}

@article{edelmanEigenvaluesConditionNumbers1988,
  title = {Eigenvalues and {{Condition Numbers}} of {{Random Matrices}}},
  author = {Edelman, Alan},
  date = {1988-10},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  volume = {9},
  number = {4},
  pages = {543--560},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0895-4798},
  doi = {10.1137/0609045},
  abstract = {Given a random matrix, what condition number should be expected? This paper presents a proof that for real or complex  𝑛×𝑛 n×n  matrices with elements from a standard normal distribution, the expected value of the log of the 2-norm condition number is asymptotic to  log𝑛 log⁡n  as  𝑛→∞ n→∞ . In fact, it is roughly  log𝑛+1.537 log⁡n+1.537  for real matrices and  log𝑛+0.982 log⁡n+0.982  for complex matrices as  𝑛→∞ n→∞ . The paper discusses how the distributions of the condition numbers behave for large n for real or complex and square or rectangular matrices. The exact distributions of the condition numbers of  2×𝑛 2×n  matrices are also given. Intimately related to this problem is the distribution of the eigenvalues of Wishart matrices. This paper studies in depth the largest and smallest eigenvalues, giving exact distributions in some cases. It also describes the behavior of all the eigenvalues, giving an exact formula for the expected characteristic polynomial.},
  keywords = {15A52,characteristic polynomial,condition number,eigenvalues,random matrices,singular values,Wishart distribution},
  file = {/Users/felix/paper/1988_Edelman/Edelman_1988_Eigenvalues and Condition Numbers of Random Matrices.pdf}
}

@article{fyodorovComplexityRandomEnergy2004,
  title = {Complexity of {{Random Energy Landscapes}}, {{Glass Transition}}, and {{Absolute Value}} of the {{Spectral Determinant}} of {{Random Matrices}}},
  author = {Fyodorov, Yan V.},
  date = {2004-06-15},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {92},
  number = {24},
  eprint = {cond-mat/0401287},
  eprinttype = {arxiv},
  pages = {240601},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.92.240601},
  abstract = {Finding the mean of the total number Ntot of stationary points for N-dimensional random energy landscapes is reduced to averaging the absolute value of the characteristic polynomial of the corresponding Hessian. For any finite N we provide the exact solution to the problem for a class of landscapes corresponding to the “toy model” of manifolds in a random environment. For N≫1 our asymptotic analysis reveals a phase transition at some critical value μc of a control parameter μ from a phase with a finite landscape complexity: Ntot∼eNΣ, Σ(μ{$<$}μc){$>$}0 to the phase with vanishing complexity: Σ(μ{$>$}μc)=0. Finally, we discuss a method of dealing with the modulus of the spectral determinant applicable to a broad class of problems.},
  archiveprefix = {arXiv},
  file = {/Users/felix/paper/2004_Fyodorov/Fyodorov_2004_Complexity of Random Energy Landscapes, Glass Transition, and Absolute Value of.pdf;/Users/felix/paper/2004_Fyodorov/Fyodorov_2004_Complexity of Random Energy Landscapes, Glass Transition, and Absolute Value of2.pdf;/Users/felix/Zotero/storage/PH78I8GF/PhysRevLett.92.html;/Users/felix/Zotero/storage/SDFWLHS5/PhysRevLett.92.html}
}

@unpublished{fyodorovHighDimensionalRandomFields2013,
  title = {High-{{Dimensional Random Fields}} and {{Random Matrix Theory}}},
  author = {Fyodorov, Yan V.},
  date = {2013-11-18},
  eprint = {1307.2379},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:math-ph},
  url = {http://arxiv.org/abs/1307.2379},
  urldate = {2022-04-12},
  abstract = {Our goal is to discuss in detail the calculation of the mean number of stationary points and minima for random isotropic Gaussian fields on a sphere as well as for stationary Gaussian random fields in a background parabolic confinement. After developing the general formalism based on the high-dimensional Kac-Rice formulae we combine it with the Random Matrix Theory (RMT) techniques to perform analysis of the random energy landscape of \$p-\$spin spherical spinglasses and a related glass model, both displaying a zero-temperature one-step replica symmetry breaking glass transition as a function of control parameters (e.g. a magnetic field or curvature of the confining potential). A particular emphasis of the presented analysis is on understanding in detail the picture of "topology trivialization" (in the sense of drastic reduction of the number of stationary points) of the landscape which takes place in the vicinity of the zero-temperature glass transition in both models. We will reveal the important role of the GOE "edge scaling" spectral region and the Tracy-Widom distribution of the maximal eigenvalue of GOE matrices for providing an accurate quantitative description of the universal features of the topology trivialization scenario.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2013_Fyodorov/Fyodorov_2013_High-Dimensional Random Fields and Random Matrix Theory.pdf;/Users/felix/Zotero/storage/CWGQ4C9V/1307.html}
}

@book{hirschDifferentialTopology1976,
  title = {Differential {{Topology}}},
  author = {Hirsch, Morris W.},
  date = {1976},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {33},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4684-9449-5},
  isbn = {978-1-4684-9451-8 978-1-4684-9449-5},
  keywords = {differential topology,Differentialtopologie,Immersion,manifold,topology},
  file = {/Users/felix/paper/1976_Hirsch/Hirsch_1976_Differential Topology.pdf}
}

@inproceedings{kawaguchiDeepLearningPoor2016,
  title = {Deep {{Learning}} without {{Poor Local Minima}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kawaguchi, Kenji},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html},
  urldate = {2022-09-02},
  abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.},
  file = {/Users/felix/paper/2016_Kawaguchi/Kawaguchi_2016_Deep Learning without Poor Local Minima.pdf}
}

@article{majumdarExtremeValueStatistics2020,
  title = {Extreme Value Statistics of Correlated Random Variables: {{A}} Pedagogical Review},
  shorttitle = {Extreme Value Statistics of Correlated Random Variables},
  author = {Majumdar, Satya N. and Pal, Arnab and Schehr, Grégory},
  date = {2020-01-22},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  series = {Extreme Value Statistics of Correlated Random Variables: {{A}} Pedagogical Review},
  volume = {840},
  pages = {1--32},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2019.10.005},
  abstract = {Extreme value statistics (EVS) concerns the study of the statistics of the maximum or the minimum of a set of random variables. This is an important problem for any time-series and has applications in climate, finance, sports, all the way to physics of disordered systems where one is interested in the statistics of the ground state energy. While the EVS of ‘uncorrelated’ variables are well understood, little is known for strongly correlated random variables. Only recently this subject has gained much importance both in statistical physics and in probability theory. In this review, we will first recall the classical EVS for uncorrelated variables and discuss the three universality classes of extreme value limiting distribution, known as the Gumbel, Fréchet and Weibull distribution. We then show that, for weakly correlated random variables with a finite correlation length/time, the limiting extreme value distribution can still be inferred from that of the uncorrelated variables using a renormalization group-like argument. Finally, we consider the most interesting examples of strongly correlated variables for which there are very few exact results for the EVS. We discuss few examples of such strongly correlated systems (such as the Brownian motion and the eigenvalues of a random matrix) where some analytical progress can be made. We also discuss other observables related to extremes, such as the density of near-extreme events, time at which an extreme value occurs, order and record statistics, etc.},
  langid = {english},
  file = {/Users/felix/paper/2020_Majumdar et al/Majumdar et al_2020_Extreme value statistics of correlated random variables.pdf;/Users/felix/Zotero/storage/43NS7XHP/S0370157319303291.html}
}

@article{mezardReplicaFieldTheory1991,
  title = {Replica Field Theory for Random Manifolds},
  author = {Mézard, Marc and Parisi, Giorgio},
  date = {1991-06-01},
  journaltitle = {Journal de Physique I},
  shortjournal = {J. Phys. I France},
  volume = {1},
  number = {6},
  pages = {809--836},
  publisher = {{EDP Sciences}},
  issn = {1155-4304, 1286-4862},
  doi = {10.1051/jp1:1991171},
  abstract = {Journal de Physique I, Journal de Physique Archives représente une mine d informations facile à consulter sur la manière dont la physique a été publiée depuis 1872.},
  langid = {english},
  file = {/Users/felix/paper/1991_Mézard_Parisi/Mézard_Parisi_1991_Replica field theory for random manifolds.pdf;/Users/felix/Zotero/storage/5NEYZNW7/jp1v1p809.html}
}

@incollection{mezardSpinGlassesIntroduction1994,
  title = {Spin {{Glasses}}: {{An Introduction}}},
  shorttitle = {Spin {{Glasses}}},
  booktitle = {From {{Statistical Physics}} to {{Statistical Inference}} and {{Back}}},
  author = {Mézard, Marc},
  editor = {Grassberger, Peter and Nadal, Jean-Pierre},
  date = {1994},
  series = {{{NATO ASI Series}}},
  pages = {183--193},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-011-1068-6_11},
  abstract = {This is a short introduction to spin glass theory for non physicists. It is not at all a review paper. More detailed presentations can be found in the references ([1]),([2]),([3]), and an excellent introduction to random systems for non specialists is ([4]). I also refer to these reviews for mentioning the original works and their references, which are systematically avoided here.},
  isbn = {978-94-011-1068-6},
  langid = {english},
  keywords = {Free Energy Density,Pure State,Spin Glass,Symmetry Breaking,Ultrametric Space}
}

@inproceedings{penningtonGeometryNeuralNetwork2017,
  title = {Geometry of {{Neural Network Loss Surfaces}} via {{Random Matrix Theory}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pennington, Jeffrey and Bahri, Yasaman},
  date = {2017-07-17},
  pages = {2798--2806},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/pennington17a.html},
  urldate = {2022-04-12},
  abstract = {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, 𝜙ϕ\textbackslash phi, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1−𝜙)21/2(1−ϕ)21/2(1-\textbackslash phi)\^2.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2017_Pennington_Bahri/Pennington_Bahri_2017_Geometry of Neural Network Loss Surfaces via Random Matrix Theory.pdf;/Users/felix/Zotero/storage/BV2L2GUY/Pennington and Bahri - 2017 - Geometry of Neural Network Loss Surfaces via Rando.pdf}
}

@article{scheuererCovarianceModelsDivergenceFree2012,
  title = {Covariance {{Models}} for {{Divergence-Free}} and {{Curl-Free Random Vector Fields}}},
  author = {Scheuerer, Michael and Schlather, Martin},
  date = {2012-07-01},
  journaltitle = {Stochastic Models},
  volume = {28},
  number = {3},
  pages = {433--451},
  publisher = {{Taylor \& Francis}},
  issn = {1532-6349},
  doi = {10.1080/15326349.2012.699756},
  abstract = {We construct matrix-valued covariance functions and in ℝ2 and ℝ3, starting from an arbitrary scalar-valued variogram. It is shown that sufficiently smooth random vector fields (RVFs) with these covariance functions have divergence-free and curl-free sample paths, respectively. Conversely, essentially all models with such sample paths can be obtained via our construction. Extensions to space-time RVFs are possible. RVFs with divergence-free and curl-free sample paths can be utilised in meteorological applications e.g. for modelling and interpolating wind fields.},
  keywords = {Curl-free,Divergence-free,Matrix-valued covariance function,Primary 60G60,Sample path properties,Secondary 60G17; 62H11; 86A32,Vector-valued random field},
  annotation = {\_eprint: https://doi.org/10.1080/15326349.2012.699756},
  file = {/Users/felix/Zotero/storage/KDSCGVKT/15326349.2012.html}
}

@unpublished{swirszczLocalMinimaTraining2017a,
  title = {Local Minima in Training of Neural Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2017-02-17},
  eprint = {1611.06310},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.06310},
  urldate = {2022-04-12},
  abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/paper/2017_Swirszcz et al/Swirszcz et al_2017_Local minima in training of neural networks2.pdf;/Users/felix/Zotero/storage/BSW5BT3W/1611.html}
}

@article{swirszczLocalMinimaTraining2022,
  title = {Local Minima in Training of Deep Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2022-07-21},
  url = {https://openreview.net/forum?id=Syoiqwcxx},
  urldate = {2022-09-02},
  abstract = {As a contribution to the discussion about error surface and the question why "deep and cheap" learning works so well we present concrete examples of local minima and obstacles arising in the...},
  langid = {english},
  file = {/Users/felix/paper/2022_Swirszcz et al/Swirszcz et al_2022_Local minima in training of deep networks.pdf;/Users/felix/Zotero/storage/QJME2G3L/forum.html}
}

@misc{taoCentralLimitTheorem2012,
  title = {A Central Limit Theorem for the Determinant of a {{Wigner}} Matrix},
  author = {Tao, Terence and Vu, Van},
  date = {2012-03-29},
  number = {arXiv:1111.6300},
  eprint = {1111.6300},
  eprinttype = {arxiv},
  primaryclass = {math},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1111.6300},
  urldate = {2022-12-20},
  abstract = {We establish a central limit theorem for the log-determinant \$\textbackslash log|\textbackslash det(M\_n)|\$ of a Wigner matrix \$M\_n\$, under the assumption of four matching moments with either the GUE or GOE ensemble. More specifically, we show that this log-determinant is asymptotically distributed like \$N(\textbackslash log \textbackslash sqrt\{n!\} - 1/2 \textbackslash log n, 1/2 \textbackslash log n)\_\textbackslash R\$ when one matches moments with GUE, and \$N(\textbackslash log \textbackslash sqrt\{n!\} - 1/4 \textbackslash log n, 1/4 \textbackslash log n)\_\textbackslash R\$ when one matches moments with GOE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {15A52,Mathematics - Probability},
  file = {/Users/felix/paper/2012_Tao_Vu/Tao_Vu_2012_A central limit theorem for the determinant of a Wigner matrix.pdf}
}

@book{williamsGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Williams, Christopher K.I. and Rasmussen, Carl Edward},
  date = {2006},
  series = {Adaptive Computation and Machine Learning},
  edition = {2},
  number = {3},
  publisher = {{MIT press Cambridge, MA}},
  url = {http://gaussianprocess.org/gpml/chapters/RW.pdf},
  isbn = {0-262-18253-X},
  langid = {english},
  pagetotal = {248},
  file = {/Users/felix/paper/2006_Williams_Rasmussen/Williams_Rasmussen_2006_Gaussian processes for machine learning2.pdf}
}
