\section{Counting Critical Points of Random Fields}

\subsection{Random Fields}

\begin{definition}[Random Field]
	A collection of random variables \((\rf(t))_{t\in\real^\dimension}\) is called
	\textbf{random field} over \(\real^\dimension\). The \textbf{covariance function}
	is defined as
	\begin{equation*}
		\C(x,y) = \Cov(\rf(x), \rf(y)).
	\end{equation*}
	A random field is called \textbf{Gaussian random field}, if all finite
	dimensional marginals are multivariate Gaussian.
	A random field is called \textbf{centered}, if for all \(t\in\real^\dimension\)
	\[
		\E[\rf(t)] = 0.
	\]
	A random field is called \textbf{strictly stationary}, if for all
	\(h\in\real^\dimension\) we the following equality in distribution
	\[
		(\rf(t+h))_{t\in\real^\dimension} \overset{d}= (\rf(t))_{t\in\real^\dimension}.
	\]
	In particular this implies	\textbf{(weak) stationarity}, i.e.
	\begin{equation*}
		\C(x,y) = \C(x-y)\qquad \text{and} \qquad \E[\rf(t)]=\mu.
	\end{equation*}
	For centered Gaussian random fields, strict and weak stationarity coincides.
	A random field is called \textbf{isotropic} (rotation invariant) if	
	\begin{equation*}
		\C(x,y) = \dimension \sqC\left(\frac{\|x-y\|^2}{2\dimension}\right).
	\end{equation*}
\end{definition}

\begin{lemma}[Covariance and Derivative]
	\label{lem: covariance of derivative}
	We have
	\begin{align*}
		\Cov(\partial_i\rf(x), \rf(y)) &= \partial_{x_i} \C(x,y)\\
		\Cov(\partial_i\rf(x), \partial_j \rf(y)) &= \partial_{x_i y_j} \C(x,y)
	\end{align*}
\end{lemma}
\begin{proof}
	\fxwarning{missing proof}	
\end{proof}
\begin{remark}\label{rem: covariance uncorrelated}
	For stationary random fields \(\C(x,y)=\C(x-y)\), the covariance
	function \(\C\) is symmetric, i.e. \(\C(h)=\C(-h)\)
	and therefore the derivative antisymmetric, i.e.
	\(\partial_i\C(-h)=-\partial_i\C(h)\). In particular
	\begin{equation*}
		\Cov(\nabla\rf(x), \rf(x)) = \nabla \C(0)=0.
	\end{equation*}
\end{remark}
\begin{corollary}[Gaussian Case] \label{cor: uncorr leads to indep in gaussian case}
	If \(\rf\) is a stationary gaussian random field, \(\rf(x)\) and
	\(\nabla\rf(x)\) are independent multivariate gaussian for every \(x\).
\end{corollary}

\subsection{Getting to the Point Process}

The set of critical points of a random field, is surprise, surprise: random.
So how would one formalize a random set in probability theory? It turns
out that this becomes much easier if we translate our random set to a random
measure. Let us consider the random set \(\Phi\)
\[
	\Phi = \{ X_1, \dots, X_K\}
\]
where \(X_1,\dots, X_K\in\real^\dimension\) are the random points of this set. Then with some
abuse of notation, we can define the measure
\[
	\Phi :
	\begin{cases}
		\borel(\real^\dimension) \to \real \\
		A \mapsto \Phi(A):= \sum_{X\in \Phi}\delta_X(A) 
	\end{cases}
\]
Where \(\delta_X(A)=\ind_A(X)\) is the dirac measure. This measure \(\Phi\)
essentially counts the number of points in the intersection \(\Phi\cap A\).
Testing with different \(A\) allows us to fully determine, what
the original points in \(\Phi\) were. So in this sense, the measure is
equivalent to the set. And it turns out that it is much more convenient
formally, to define random measures, than it is to define random sets.
So let us introduce some nomenclature for measures.

A measure \(\mu\) is called
\begin{itemize}
	\item \textbf{locally finite}, if \(\mu(A) < \infty\) for all bounded
	\(A\in\borel(\real^\dimension)\),
	\item \textbf{counting measure} if it is discrete,
	\item \textbf{simple}, if \(\mu(\{x\})\in\{0,1\}\).
\end{itemize}
Let \(\locFiniteMeasure\) be the \textbf{set of all locally finite counting measures}
on \(\real^\dimension\) and \(\locFiniteMeasAlg\) the smallest \(\sigma\)-algebra
such that
\[
	\pi_B : \begin{cases}
		\locFiniteMeasure \to \borel(\real)\\
		\mu \mapsto \mu(B)
	\end{cases}
\]
is measurable for every \(B\in\borel(\real^\dimension)\). I.e.
\[
	\locFiniteMeasAlg := \sigma(\pi_B : B\in\borel(\real^\dimension))
\]

\begin{definition}[Point Process]
	A \textbf{point process} is a measurable mapping \(\Phi:\Omega\to\locFiniteMeasure\)
	from a probability space \((\Omega, \mathcal{A}, \Pr)\) into the set of
	locally finite counting measures. Due to the definition of \(\locFiniteMeasAlg\)
	\[
		\Phi(B) := \pi_B\circ \Phi : \Omega \to \real
	\]
	is a random variable for any \(B\in\borel(\real^\dimension)\). If
	\[
		\Lambda:\begin{cases}
			\borel(\real^\dimension)\to [0,\infty)\\
			B \mapsto \E[\Phi(B)]
		\end{cases}
	\]
	is locally finite, then it is called the \textbf{intensity measure}
	(expectation measure) of \(\Phi\). The intensity measure essentially tells
	us, how many points we should expect in some set \(B\) on average.
	If \(\Lambda(dx) = \lambda dx\) for some
	constant \(\lambda\in\real\), then \(\lambda\) is called the
	\textbf{intensity} of \(\Phi\).
	A point process is called \textbf{simple}, if \(\Phi(\omega)\) is simple
	for every \(\omega\in\Omega\).
\end{definition}

Often our random set of points has a certain structure. E.g. we have
random points in space \(X_1,\dots, X_K\) with marks \(M_1,\dots, M_k\) which
might for example represent the value of some random field \(\rf\) at our
points in space, i.e. \(M_i=\rf(X_i)\). Then our random graph 
\[
	\Phi = \{ (X_1, M_1), \dots, (X_K, M_K) \}
\]
is just a point process in higher dimensional space, with points \((X_i, M_i)\).
But of course not every point process can be interpreted as a point with a mark.
So we define the special case

\begin{definition}[Marked Point Process]
	A point process \(\Phi\) on \(\locFiniteMeasure[\dimension+n]\) is called
	\textbf{marked point process}, if \(\Phi_p = \Phi(\cdot \times \real^n)\)
	is a point process on \(\locFiniteMeasure\).

	A marked point process is called \textbf{stationary}, if for all
	\(h\in\real^\dimension\), \(A_i\in\borel(\real^\dimension)\),
	\(M_i\in\borel(\real^n)\) and \(B_i\subseteq\nat_0\) we have
	\[
		\Pr(\Phi(A_i\times M_i) \in B_i, i=1,2,\dots)
		= \Pr(\Phi(A_i+h \times M_i)\in B_i, i=1,2,\dots)
	\]
\end{definition}

Notice that for a marked point process to be stationary, we only shift the
spacial part of the point. This stands in contrast to a stationarity definition
for ordinary point processes. This is the main reason we introduce this
additional machinery. In our case we are interested in the height (mark) of
our random field at our critical points. And the following Lemma will allow us
to recover the distribution of marks on these critical points

\begin{lemma}[Campbell]\label{lem: Campbell}
	For a stationary marked point process \(\Phi\) with intensity \(\Lambda\),
	we have	
	\[
		\Lambda(A \times L) = \lambda(L) |A|
	\]
	where \(|A|\) is the lebesgue measure of \(A\). We call
	\[
		\Pr_M(L) = \frac{\lambda(L)}{\lambda(\real^n)}
	\]
	the \textbf{distribution of marks}, because we have for integrable \(f\)
	(Campbell)
	\[
		\E\left[\int f(x,m) \Phi(dx,dm)\right]
		= \lambda \int \int f(x,m) \Pr_M(dm) dx
	\]
\end{lemma}
\begin{proof}
	\fxwarning{TODO: Fakt in 3.5 räumliche statistik skript schlather}
\end{proof}

\subsection{Making it Count}

\begin{definition}[Counting Critical Points]
	Let \(\negEV(H)\) be the number of negative eigenvalues of
	\(H\in\real^{\dimension\times\dimension}\), let \(\rf\) be a random field.
	Then we implicitly\footnote{
		\(\borel(\real^\dimension)\times\borel(\real)\) is a semiring. So by
		Carathéodory's extension theorem there is a unique extension of
		\(\crit^\alpha\) to
		\(\sigma(\borel(\real^\dimension)\times\borel(\real))=\borel(\real^{\dimension+1})\).
		Use this extension to obtain \(\crit^\alpha(\omega)\in\locFiniteMeasure[\dimension+1]\).
	} define the marked point process of critical points as
	\begin{align*}
		\crit^\alpha:
		\begin{cases}
			\borel(\real^\dimension)\times\borel(\real) \to \real\\
			(\Vol, A) \mapsto
			\#\left\{t\in\Vol:
				\nabla\rf(t)=0,
				\frac{\rf(t)}{\dimension}\in A,
				\negEV(\nabla^2 \rf(t))=\dimension\alpha
			\right\}.
		\end{cases}
	\end{align*}
	Here the critical points are the points and the height \(\frac{\rf(t)}\dimension\)
	are the marks. For \(\alpha=0\) we are counting the critical points where all
	eigenvalues are positive. In other words: \(\crit^0\) are the minima,
	\(\crit^1\) are the maxima, all others are saddle points.

	Finally we can define the point process over all critical points
	\[
		\crit(\Vol, A)
		:= \sum_{k=0}^\dimension \crit^{\frac{k}\dimension}(\Vol, A)
	\]
\end{definition}
\begin{proof}[Proof (Measurability)]
	First note that	
	\begin{align*}
		\locFiniteMeasAlg[\dimension+1]
		&= \sigma(\pi_A : A\in\borel(\real^{\dimension+1}))\\
		&=\sigma(\underbrace{
			\{\pi_A^{-1}(B) : A\in\borel(\real^{\dimension+1}), B\in\borel(\real)\}
		}_{=:\mathcal{E}}
		)
	\end{align*}
	since it is sufficient to prove measurability on the generator
	\(\mathcal{E}\), we only need that the following sets are measurable
	\begin{align*}
		(\crit^\alpha)^{-1}(\pi^{-1}_{ A }(B))
		&= (\pi_{A} \circ \crit^\alpha)^{-1}(B)
		= (\crit^\alpha(A))^{-1}(B)\\
		&= \#\left\{\left(t,\frac{\rf(t)}{\dimension}\right) \in A:
			\nabla\rf(t)=0,
			\negEV(\nabla^2 \rf(t))=\dimension\alpha
		\right\}\\
		&= \lim_{\epsilon\to 0} \sum_{k\in I^\epsilon}
		\ind\left\{
			\inf_{t\in \left(\text{id}, \frac{\rf}\dimension, \negEV(\nabla^2\rf)\right)^{-1}(A^\epsilon_k\times \{\dimension\alpha\}) }
		|\nabla \rf(t)| = 0
		\right\}
	\end{align*}
	\fxwarning{probably not quite right yet}
	where \((A^\epsilon_k)_{k\in\nat}\) is an \(\epsilon\) tiling of \(A\) (countable).
	We can swap limit with series due to monotone convergence.
\end{proof}

\begin{lemma}[Stationarity]
	If \(\rf\) is strictly stationary, \(\crit^\alpha\) is stationary.
\end{lemma}
\begin{proof}
	Let \(h\in \real^\dimension\) and note that
	\[
		\tilde{\rf}(t) := \rf(t+h)
	\]
	has the same distribution as \(\rf\) since \(\rf\) is strictly stationary. Define
	\(\tilde{\crit}^\alpha\) using \(\tilde{\rf}\) in place of \(\rf\). Then
	\(\tilde{\crit}^\alpha\) has the same distribution as \(\crit^\alpha\).
	But since we have
	\begin{align*}
		&\crit^\alpha(\Vol+h, M)\\
		&= \#\left\{t\in\Vol+h:
			\nabla\rf(t)=0,
			\frac{\rf(t)}{\dimension}\in M,
			\negEV(\nabla^2 \rf(t))=\dimension\alpha
		\right\}\\
		&= \#\left\{t\in\Vol:
			\nabla\tilde{\rf}(t)=0,
			\frac{\tilde{\rf}(t)}{\dimension}\in M,
			\negEV(\nabla^2 \tilde{\rf}(t))=\dimension\alpha
		\right\}\\
		&= \tilde{\crit}^\alpha(\Vol, M).
	\end{align*}
	So \(\crit^\alpha\) is stationary:
	\begin{align*}
		\Pr\left(\crit^\alpha((\Vol_i+h)\times M_i)\in B_i, i=1,2,\dots\right)
		&= \Pr\left(\tilde{\crit}^\alpha(\Vol_i\times M_i)\in B_i, i=1,2,\dots\right)\\
		&= \Pr\left(\crit^\alpha(\Vol_i\times M_i)\in B_i, i=1,2,\dots\right).
		\qedhere
	\end{align*}
\end{proof}

To count critical points with restrictions on the second derivative, the second
derivative has to exist. For that, note that
\[
	\partial_{x_i x_j, y_i y_j}\C(x,y) = \Cov(\partial_{ij}\rf(x), \partial_{ij}(y)),
\]
and it turns out that existence of the term on the left essentially ensures
existence of \(\partial_{ij}\rf\)\fxnote{reference}. For continuous second
derivatives, we want a little bit more \parencite[cf.
Theorem~1.4.1]{adlerRandomFieldsGeometry2007}. In fact, we want the following:

\begin{assumption}[Smooth Random Field]\label{assmpt: smoothness assumption}
	A sufficient condition for smooth second derivatives \(\partial_{ij}	\rf\)
	of a random field, is
	\begin{equation}\label{eq: sufficient for continuous second derivative}
		\max_{i,j}\E[|\partial_{ij}\rf(t) - \partial_{ij}\rf(s)|^2]
		\le K |\ln(|t-s|)|^{-(1+\alpha)}
	\end{equation}
	for some \(K>0\), \(\alpha>0\) for all \(|t-s|\) small enough.
	In the stationary case this condition simplifies to
	\[
		\max_{i,j}|\partial_{ii jj}\C(0)
		-\partial_{ii jj}\C(h)| \le \frac{K}2 |\ln(|h|)|^{-(1+\alpha)}
	\]
	for \(h\in\real^\dimension\) with \(|h|\) small enough.
\end{assumption}


\begin{theorem}[Kac-Rice Formula]
	For a centered Gaussian random field \(\rf\) satisfying our smoothness
	Assumption~\ref{assmpt: smoothness assumption}, the intensity measure of the
	critical points \(\crit^\alpha\) with index \(\dimension\alpha\) is given by
	\begin{align*}
		&\Lambda^\alpha(\Vol,A)
		=\E[\crit^\alpha(\Vol, A)]\\
		&= \int_\Vol \E\left[
			\left|\det(\nabla^2 \rf(t))\right|
			\ind_{\frac{\rf(t)}{\dimension}\in A} \ind_{\negEV(\nabla^2 \rf(t))=\dimension \alpha}
			\bigm| \nabla\rf(t) = 0 
		\right] \density_{\nabla\rf(t)}(0)dt
		\\
		\overset{\text{stationary}}&=
		|\Vol|\ \underbrace{\E\left[
			|\det(\nabla^2 \rf(0))|
			\ind_{\frac{\rf(0)}\dimension\in A} \ind_{\negEV(\nabla^2 \rf(0))=\dimension \alpha}
		\right] \density_{\nabla\rf(0)}(0)}_{=:\lambda(A)}
	\end{align*}
	where \(\density_{\nabla\rf(t)}\) is the density of \(\nabla\rf(t)\). The
	second equality is only true when \(\rf\) is stationary.
\end{theorem}

\begin{proof}
	This is an immediate result from Corollary~\ref{cor: gaussian kac-rice}
	\parencite[Corollary~11.2.2]{adlerRandomFieldsGeometry2007}. More precisely
	we define
	\begin{align*}
		f(t) &:= \nabla \rf(t)\\
		g(t) &:= (\rf(t), \nabla^2 \rf(t))
	\end{align*}
	We now simply need to check that the covariance function of the components of
	\(\partial_j f^i(t) = \partial_{ji} \rf(t)\) and \(g^i\) (which are
	also just the second derivatives and \(\rf\) itself) satisfy the smoothness
	requirement of the corollary. As smoothness of the second derivative implies
	smoothness of \(\rf\), we only need to check the requirement for \(\partial_{ji} \rf\).
	In other words, we need
	\[
		\max_{i,j}|\partial_{x_i x_j, y_i y_j}\C(t,t)
		+ \partial_{x_i x_j, y_i y_j}\C(s,s)
		-2\partial_{x_i x_j, y_i y_j}\C(s,t)| \le K |\ln(|t-s|)|^{-(1+\alpha)}
	\]
	But that is \eqref{eq: sufficient for continuous second derivative} in
	Assumption~\ref{assmpt: smoothness assumption} written in terms of
	covariances. By application
	of Corollary~\ref{cor: gaussian kac-rice}, we therefore get\footnote{
		Careful readers might protest about the fact that \(\negEV^{-1}(\{\dimension
		\alpha\})\) is not an open set and therefore \(B:=
		A\times\negEV^{-1}(\{\dimension\alpha\})\) can not fulfill the requirements
		of the Corollary. To fix this, we simply define a new point process
		\[
			\tilde{\crit}(\Vol, A, C)
			:= \#\{t\in\Vol: \nabla \rf(t) = 0, \rf(t)\in \dimension A, \nabla^2\rf(t)\in C\}
		\]
		where \(\Vol\) is a closed cuboid and \(A,C\) are open cuboids. As the cuboids
		are a generator of the Borel sets, we can then uniquely extend this point
		process measure to all Borel sets. Similarly we define \(\tilde{\Lambda}\),
		and similarly we can extend it to all Borel sets. But with this extension we
		can now simply set \(C:=\negEV^{-1}(\{\dimension\alpha\})\) which is a Borel
		set, since the eigenvalues are continuous in the entries of the matrix and
		therefore the number of negative eigenvalues a measurable function.
	}
	\begin{align*}
		\Lambda(\Vol, A) 
		&= \E[\crit^\alpha(\Vol, A)]
		= \E[\level(f, g: \Vol, A\times \negEV^{-1}(\{\dimension \alpha\}))]\\
		&= \int_\Vol \E\left[
			|\det \nabla f(t)| \ind_A(\rf(t)) \ind_{\{\dimension \alpha\}}(\negEV(\nabla^2\rf(t))) \mid f(t) = u
		\right] p_t(u)dt.
	\end{align*}
	
	
	The second inequality follows as \(\nabla\rf(t)\) is independent of
	\((\rf(t),\nabla^2\rf(t))\) due to Corollary~\ref{cor: uncorr leads to indep
	in gaussian case} which turns the conditional expectation into a normal
	expectation. Lastly replace all \(t\) with \(0\) due to stationarity.
\end{proof}
\begin{remark}
	As can be seen in Theorem~\ref{thm: general kac-rice} \parencite[Theorem
	11.2.1]{adlerRandomFieldsGeometry2007}, \(\rf\) does not necessarily
	need to be Gaussian. We just need it and its derivatives to satisfy a
	list of properties which are satisfied for Gaussian process with
	smooth enough covariance functions.
\end{remark}

\begin{corollary}
	For a centered, stationary Gaussian random field \(\rf\), the probability
	density of \(\Pr_{\rf\mid\text{crit}(\alpha)}\) is up to a normalizing
	constant
	\begin{equation*}
		\Omega^\alpha(\epsilon)
		= \int |\det(H)| \ind_{\negEV(H) = \dimension\alpha} p(H, \dimension\epsilon)dH,
	\end{equation*}
	where \(p\) is the joint density of \((\nabla^2 \rf(0), \rf(0))\).
	More precisely
	\[
		\Pr_{\rf\mid\text{crit}(\alpha)}(A)
		= \frac{\int_A \Omega^\alpha(\epsilon)d\epsilon}{\int \Omega^\alpha(\epsilon)d\epsilon}.
	\]
\end{corollary}
\begin{proof}
	With Lemma~\ref{lem: Campbell}, we have the height distribution of critical
	points with index \(\alpha\)
	\begin{align*}
		\Pr_{\rf\mid \text{crit}(\alpha)}(A)
		&= \frac{\lambda(A)}{\lambda(\real)}
		= \frac{
			\E[|\det(\nabla^2 \rf(0))|
			\ind_{\negEV(\nabla^2\rf(0))=\dimension\alpha}
			\ind_{\frac{\rf(0)}\dimension \in A}
			]
		}{
			\E[|\det(\nabla^2 \rf(0))|\ind_{\negEV(\nabla^2\rf(0))=\dimension\alpha}]
		}\\
		&= \frac{
			\int \int_A p(H, \dimension \epsilon) |\det(H)|
			\ind_{\negEV(H)=\dimension\alpha}d\epsilon dH
		}{
			\int \int p(H, \dimension \epsilon) |\det(H)|
			\ind_{\negEV(H)=\dimension\alpha}d\epsilon dH
		}
		\qedhere
	\end{align*}
\end{proof}