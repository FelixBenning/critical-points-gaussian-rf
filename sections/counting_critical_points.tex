\section{Counting Critical Points of Random Fields}

\subsection{Random Fields}

\begin{definition}[Random Field]
	A collection of random variables \((\rf(t))_{t\in\real^\dimension}\) is called
	\textbf{random field} over \(\real^\dimension\). The \textbf{covariance function}
	is defined as
	\begin{equation*}
		\C(x,y) = \Cov(\rf(x), \rf(y)).
	\end{equation*}
	A random field is called \textbf{Gaussian random field}, if all finite
	dimensional marginals are multivariate Gaussian.
	A random field is called \textbf{centered}, if for all \(t\in\real^\dimension\)
	\[
		\E[\rf(t)] = 0.
	\]
	A random field is called \textbf{strictly stationary}, if for all
	\(h\in\real^\dimension\) we have the following equality in distribution
	\[
		(\rf(t+h))_{t\in\real^\dimension} \overset{d}= (\rf(t))_{t\in\real^\dimension}.
	\]
	In particular this implies	\textbf{(weak) stationarity}, i.e.
	\begin{equation*}
		\C(x,y) = \C(x-y)\qquad \text{and} \qquad \E[\rf(t)]=\mu.
	\end{equation*}
	For Gaussian random fields, strict and weak stationarity coincides.
	A random field is called \textbf{isotropic} (rotation invariant) if	
	\begin{equation*}
		\C(x,y) = \sqC\left(\frac{\|x-y\|^2}{2}\right).
	\end{equation*}
\end{definition}

\subsubsection{Differentiability}

\begin{lemma}[Covariance and Derivative]
	\label{lem: covariance of derivative}
	Assume the partial derivative \(\partial_i\rf(x)\) to exists almost surely and
	\(x\mapsto\Var(\partial_i\rf(x))\) to be continuous in some neighborhood of
	\(x\). Then we have
	\begin{equation*}
		\Cov(\partial_i\rf(x), \rf(y)) = \partial_{x_i} \C(x,y).
	\end{equation*}
	Since \(\partial_i\rf(x)\) is just a random field again, we can iterate
	on this idea to get the covariance between \(\partial_i\rf(x)\) and
	\(\partial_j\rf(y)\) or higher order derivatives \(\partial_{ij}\rf(x)\).
\end{lemma}
\begin{proof}
	Assume without loss of generality \(Z\) is centered. To apply
	Theorem~\ref{thm: swap integration and differentiation} we need
	\(x\mapsto\E[|\partial_i\rf(x)\rf(y)|]\) to be integrable in a local neighborhood.
	But by the HÃ¶lder inequality, we have
	\[
		\E[|\partial_i\rf(x)\rf(y)|]
		\le \E[\partial_i\rf(x)^2]^{\tfrac12} 
		\underbrace{\E[\rf(y)^2]^{\tfrac12}}_{=\sqrt{\C(y,y)}}
	\]
	and we assumed \(x\mapsto\E[\partial_i\rf(x)^2]\) to be continuous. It is therefore
	locally integrable and we have
	\begin{align*}
		\Cov(\partial_i \rf(x), \rf(y))
		&= \int \partial_{x_i}\rf(x,\omega) \rf(y,\omega) d\Pr(\omega)
		\overset{\ref{thm: swap integration and differentiation}}= \partial_{x_i} \int \rf(x,\omega) \rf(y,\omega) d\Pr(\omega)\\
		&= \partial_{x_i} \C(x,y).
		\qedhere
	\end{align*}
\end{proof}
Since we want
\[
	\Cov(\partial_i \rf(x), \partial_j\rf(y)) = \partial_{x_i}\partial_{y_i}\C(x,y)
	\overset{\text{stationary case}}= -\partial_{ij}\C(x-y),
\]
it is natural that the existence of this second order derivative of \(\C\) is
necessary for the existence of directional derivatives of \(\rf\). It turns out
that it is also almost sufficient. If \(\C\) is twice differentiable, then
\[
	\Delta_v^n \rf(x)	:= \frac{\rf(x+\frac{v}n)-\rf(x-\frac{v}n)}{2/n}
\]
is an \(L^2\) cauchy sequence and its \(L^2\) limit therefore exists and is
almost surely unique. In particular we can now define
\[
	\partial_i\rf(x):=\lim_{n\to\infty} \Delta_{e_i}^n\rf(x)
\]
in the \(L^2\) sense. Whenever \(\rf\) is Gaussian \((\Delta_v^n \rf(x),
\rf(y))\) is multivariate Gaussian, and therefore also its limit.

But we want almost sure point-wise limits, not \(L^2\) limits. To recover this
property, we can use a trick from 
\textcite[Sec.~1.4.2]{adlerRandomFieldsGeometry2007}. We define the random field
\[
	F: \begin{cases}
		\real^\dimension \times \real^\dimension \times \real \to \real\\
		(x,v,h)	\mapsto \frac{\rf(x+hv)-\rf(x-hv)}{2h}
	\end{cases}
\]
where \(\Delta_i^n Z(x) = F(x,e_i,\tfrac1n)\). Technically \(F(x,v,0)\) is not
well defined right now. In its place, we insert the appropriate directional
derivative (which exists in the \(L^2\) sense). If this insertion leaves \(F\)
to be almost surely continuous, then we have almost surely
\[
	\Delta_i^n Z(x)=F(x,e_i,\tfrac1n)\to F(x, e_i, 0)= \partial_i Z(x).
\]
we obtain for free, that \(x\mapsto \partial_i Z(x) = F(x,e_i,0)\) is almost
surely continuous. So we only need a condition for random fields to be almost
surely continuous and simply apply this to \(F\) instead of \(\rf\). Higher
order derivatives work similarly. For details see
\textcite[Sec.~1.4.2]{adlerRandomFieldsGeometry2007}.

To count critical points with restrictions on the second derivative, we need
the second derivative to exist. And continuity would be nice. The following
assumption is sufficient for both \parencite*[cf.
Theorem~1.4.2]{adlerRandomFieldsGeometry2007}

\begin{assumption}[Smooth Random Field]\label{assmpt: smoothness assumption}
	\fxwarning{Copied wrong theorem so far}
	A sufficient condition for smooth second derivatives \(\partial_{ij}	\rf\)
	of a random field, is
	\begin{equation}\label{eq: sufficient for continuous second derivative}
		\max_{i,j}\E[|\partial_{ij}\rf(t) - \partial_{ij}\rf(s)|^2]
		\le K |\ln(|t-s|)|^{-(1+\alpha)}
	\end{equation}
	for some \(K>0\), \(\alpha>0\) for all \(|t-s|\) small enough.
	In the stationary case this condition simplifies to
	\[
		\max_{i,j}|\partial_{ii jj}\C(0)
		-\partial_{ii jj}\C(h)| \le \frac{K}2 |\ln(|h|)|^{-(1+\alpha)}
	\]
	for \(h\in\real^\dimension\) with \(|h|\) small enough.
\end{assumption}


\begin{remark}\label{rem: covariance uncorrelated}
	For stationary random fields \(\C(x,y)=\C(x-y)\), the covariance
	function \(\C\) is symmetric, i.e. \(\C(h)=\C(-h)\)
	and therefore the derivative antisymmetric, i.e.
	\(\partial_i\C(-h)=-\partial_i\C(h)\). In particular
	\begin{equation*}
		\Cov(\nabla\rf(x), \rf(x)) = \nabla \C(0)=0.
	\end{equation*}
\end{remark}
\begin{corollary}[Gaussian Case] \label{cor: uncorr leads to indep in gaussian case}
	If \(\rf\) is a stationary gaussian random field, \(\rf(x)\) and
	\(\nabla\rf(x)\) are independent multivariate gaussian for every \(x\).
\end{corollary}

\subsection{Getting to the Point Process}

The set of critical points of a random field, is surprise, surprise: random.
So how would one formalize a random set in probability theory? It turns
out that this becomes much easier if we translate our random set to a random
measure. Let us consider the random set \(\Phi\)
\[
	\Phi = \{ X_1, \dots, X_K\}
\]
where \(X_1,\dots, X_K\in\real^\dimension\) are the random points of this set. Then with some
abuse of notation, we can define the measure
\[
	\Phi :
	\begin{cases}
		\borel(\real^\dimension) \to \real \\
		A \mapsto \Phi(A):= \sum_{X\in \Phi}\delta_X(A) 
	\end{cases}
\]
Where \(\delta_X(A)=\ind_A(X)\) is the dirac measure. This measure \(\Phi\)
essentially counts the number of points in the intersection \(\Phi\cap A\).
Testing with different \(A\) allows us to fully determine, what
the original points in \(\Phi\) were. So in this sense, the measure is
equivalent to the set. And it turns out that it is much more convenient
formally, to define random measures, than it is to define random sets.
So let us introduce some nomenclature for these specific measures.

We want our point-counting measure \(\mu\) to be
\begin{itemize}
	\item \textbf{locally finite}, i.e. \(\mu(A) < \infty\) for all bounded
	\(A\in\borel(\real^\dimension)\),
	\item a \textbf{counting measure}, i.e. it should be discrete,
	\item \textbf{simple}, i.e. \(\mu(\{x\})\in\{0,1\}\), if we do not want
	to allow duplicate points.
\end{itemize}
Let \(\locFiniteMeasure\) be the \textbf{set of all locally finite counting measures}
on \(\real^\dimension\) and \(\locFiniteMeasAlg\) the smallest \(\sigma\)-algebra
such that
\[
	\pi_B : \begin{cases}
		\locFiniteMeasure \to \borel(\real)\\
		\mu \mapsto \mu(B)
	\end{cases}
\]
is measurable for every \(B\in\borel(\real^\dimension)\). I.e.
\[
	\locFiniteMeasAlg := \sigma(\pi_B : B\in\borel(\real^\dimension))
\]

\begin{definition}[Point Process]
	A \textbf{point process} is a measurable mapping \(\Phi:\Omega\to\locFiniteMeasure\)
	from a probability space \((\Omega, \mathcal{A}, \Pr)\) into the set of
	locally finite counting measures. Due to the definition of \(\locFiniteMeasAlg\)
	\[
		\Phi(B) := \pi_B\circ \Phi : \Omega \to \real
	\]
	is a random variable for any \(B\in\borel(\real^\dimension)\). If
	\[
		\Lambda:\begin{cases}
			\borel(\real^\dimension)\to [0,\infty)\\
			B \mapsto \E[\Phi(B)]
		\end{cases}
	\]
	is locally finite, then it is called the \textbf{intensity measure}
	(expectation measure) of \(\Phi\). The intensity measure essentially tells
	us, how many points we should expect in some set \(B\) on average.
	If \(\Lambda(dx) = \lambda dx\) for some
	constant \(\lambda\in\real\), then \(\lambda\) is called the
	\textbf{intensity} of \(\Phi\).
	A point process is called \textbf{simple}, if \(\Phi(\omega)\) is simple
	for every \(\omega\in\Omega\).
\end{definition}

Often our random set of points has a certain structure. E.g. we have
random points in space \(X_1,\dots, X_K\) with marks \(M_1,\dots, M_k\) which
might for example represent the value of some random field \(\rf\) at our
points in space, i.e. \(M_i=\rf(X_i)\). Then our random graph 
\[
	\Phi = \{ (X_1, M_1), \dots, (X_K, M_K) \}
\]
is just a point process in higher dimensional space, with points \((X_i, M_i)\).
But of course not every point process can be interpreted as a point with a mark.
So we define the special case

\begin{definition}[Marked Point Process]
	A point process \(\Phi\) on \(\locFiniteMeasure[\dimension+n]\) is called
	\textbf{marked point process}, if \(\Phi_p = \Phi(\cdot \times \real^n)\)
	is a point process on \(\locFiniteMeasure\).

	A marked point process is called \textbf{stationary}, if for all
	\(h\in\real^\dimension\), \(A_i\in\borel(\real^\dimension)\),
	\(M_i\in\borel(\real^n)\) and \(B_i\subseteq\nat_0\) we have
	\[
		\Pr(\Phi(A_i\times M_i) \in B_i, i=1,2,\dots)
		= \Pr(\Phi(A_i+h \times M_i)\in B_i, i=1,2,\dots)
	\]
\end{definition}

Notice that for a marked point process to be stationary, we only shift the
spacial part of the point. This stands in contrast to a stationarity definition
for ordinary point processes. This is the main reason we introduce this
additional machinery. In our case we are interested in the height (mark) of
our random field at our critical points. And the following Lemma will allow us
to recover the distribution of marks on these critical points

\begin{lemma}[Campbell]\label{lem: Campbell}
	For a stationary marked point process \(\Phi\) with intensity \(\Lambda\),
	we have	
	\[
		\Lambda(\Vol \times L) = \lambda(L) |\Vol|
	\]
	where \(|\Vol|\) is the lebesgue measure of \(\Vol\). We call
	\[
		\Pr_M(L) = \frac{\lambda(L)}{\lambda(\real^n)}
	\]
	the \textbf{distribution of marks}, because we have for integrable \(f\)
	(Campbell)
	\[
		\E\left[\int f(x,m) \Phi(dx,dm)\right]
		= \lambda \int \int f(x,m) \Pr_M(dm) dx
	\]
\end{lemma}
\begin{proof}
	\fxwarning{TODO: Fakt in 3.5 rÃ¤umliche statistik skript schlather}
\end{proof}

\subsection{Making it Count}

\begin{definition}[Counting Critical Points]
	Let \(\rf\) be a random field. We implicitly\footnote{
		\(\borel(\real^\dimension)\times\borel(\real^{1+\dimension^2})\) is a semiring. So by
		CarathÃ©odory's extension theorem there is a unique extension of
		\(\crit\) to
		\(\sigma(\borel(\real^\dimension)\times\borel(\real^{1+\dimension^2}))=\borel(\real^{\dimension+1+\dimension^2})\).
		Use this extension to obtain \(\crit(\omega)\in\locFiniteMeasure[\dimension+1+\dimension^2]\).
	} define the marked point process of critical points as
	\begin{align*}
		\crit:
		\begin{cases}
			\borel(\real^\dimension)\times\borel(\real^{1+\dimension^2}) \to \real\\
			(\Vol, M) \mapsto
			\#\left\{t\in\Vol:
				\nabla\rf(t)=0,
				(\rf(t), \nabla^2 \rf(t))\in M
			\right\}.
		\end{cases}
	\end{align*}
	This is a marked point process with \((\rf(t), \nabla^2\rf(t))\) as marks.
	Let \(\negEV(H)\) be the number of negative eigenvalues\footnote{
		Since the eigenvalues are continuous in the entries of the matrix, the
		number of negative eigenvalues is a measurable function.
	} of \(H\in\real^{\dimension\times\dimension}\). Then, the number of minima in
	\(\Vol\) with height \(\rf(t)\) in \(A\) can be expressed as
	\[
		\crit^0(\Vol, A):= \crit(\Vol, A\times\negEV^{-1}(0)).
	\]
	More generally we define the point-process of critical points with index
	\(\alpha\) (the share of negative eigenvalues) as
	\[
		\crit^\alpha(\Vol, A)
		:= \crit(\Vol, A\times\negEV^{-1}(\dimension\alpha)).
	\]
\end{definition}
\begin{proof}[Proof (Measurability)]
	First note that	
	\begin{align*}
		\locFiniteMeasAlg[\dimension+1+\dimension^2]
		&= \sigma(\pi_B : B\in\borel(\real^{\dimension+1+\dimension^2}))\\
		&=\sigma(\underbrace{
			\{\pi_B^{-1}(C) : B\in\borel(\real^{\dimension+1+\dimension^2}), C\in\borel(\real)\}
		}_{=:\mathcal{E}}
		)
	\end{align*}
	since it is sufficient to prove measurability on the generator
	\(\mathcal{E}\), we only need that the following sets are measurable
	\begin{align*}
		(\crit)^{-1}(\pi^{-1}_{B}(C))
		&= (\pi_{B} \circ \crit)^{-1}(C)
		= (\crit(B))^{-1}(C)\\
		&= \#\left\{\left(t,\rf(t),\nabla^2\rf(t)\right) \in B:
			\nabla\rf(t)=0
		\right\}\\
		&= \lim_{\epsilon\to 0} \sum_{k\in I^\epsilon}
		\ind\left\{
			\inf_{t\in \left(\text{id}, \rf, \nabla^2\rf\right)^{-1}(B^\epsilon_k) }
		|\nabla \rf(t)| = 0
		\right\}
	\end{align*}
	\fxwarning{probably not quite right yet}
	where \((B^\epsilon_k)_{k\in\nat}\) is an \(\epsilon\) tiling of \(B\) (countable).
	We can swap the limit with the series due to monotone convergence.
\end{proof}

\begin{lemma}[Stationarity]
	If \(\rf\) is strictly stationary, \(\crit\) is stationary.
\end{lemma}
\begin{proof}
	Let \(h\in \real^\dimension\) and note that
	\[
		\tilde{\rf}(t) := \rf(t+h)
	\]
	has the same distribution as \(\rf\) since \(\rf\) is strictly stationary. Define
	\(\tilde{\crit}\) using \(\tilde{\rf}\) in place of \(\rf\). Then
	\(\tilde{\crit}\) has the same distribution as \(\crit\).
	And we have
	\begin{align*}
		&\crit(\Vol+h, M)\\
		&= \#\left\{t\in\Vol+h:
			\nabla\rf(t)=0,
			(\rf(t),\nabla^2\rf(t))\in M
		\right\}\\
		&= \#\left\{t\in\Vol:
			\nabla\tilde{\rf}(t)=0,
			(\tilde{\rf}(t),\nabla^2\tilde{\rf}(t))\in M
		\right\}\\
		&= \tilde{\crit}(\Vol, M).
	\end{align*}
	So \(\crit\) is stationary:
	\begin{align*}
		\Pr\left(\crit((\Vol_i+h)\times M_i)\in B_i, i=1,2,\dots\right)
		&= \Pr\left(\tilde{\crit}(\Vol_i\times M_i)\in B_i, i=1,2,\dots\right)\\
		&= \Pr\left(\crit(\Vol_i\times M_i)\in B_i, i=1,2,\dots\right).
		\qedhere
	\end{align*}
\end{proof}


\begin{theorem}[Kac-Rice Formula]
	\label{thm: kac-rice formula}
	For a centered Gaussian random field \(\rf\) satisfying our smoothness
	Assumption~\ref{assmpt: smoothness assumption}, the intensity measure of the
	critical points \(\crit\) is given by
	\begin{align*}
		&\Lambda(\Vol,M)
		=\E[\crit(\Vol, M)]\\
		&= \int_\Vol \E\left[
			\left|\det(\nabla^2 \rf(t))\right|
			\ind_M(\rf(t), \nabla^2\rf(t))
			\bigm| \nabla\rf(t) = 0 
		\right] \density_{\nabla\rf(t)}(0)dt
		\\
		\overset{\text{stationary}}&=
		|\Vol|\ \underbrace{\E\left[
			|\det(\nabla^2 \rf(0))|
			\ind_M(\rf(0), \nabla^2\rf(0))
		\right] \density_{\nabla\rf(0)}(0)}_{=:\lambda(M)}
	\end{align*}
	where \(\density_{\nabla\rf(t)}\) is the density of \(\nabla\rf(t)\). The
	second equality is only true when \(\rf\) is stationary.
\end{theorem}

\begin{proof}
	This is an immediate result from Corollary~\ref{cor: gaussian kac-rice}
	\parencite[Corollary~11.2.2]{adlerRandomFieldsGeometry2007}. More precisely
	we define
	\begin{align*}
		f(t) &:= \nabla \rf(t)\\
		g(t) &:= (\rf(t), \nabla^2 \rf(t))
	\end{align*}
	We now simply need to check that the covariance function of the components of
	\(\partial_j f^i(t) = \partial_{ji} \rf(t)\) and \(g^i\) (which are
	also just the second derivatives and \(\rf\) itself) satisfy the smoothness
	requirement of the corollary. As smoothness of the second derivative implies
	smoothness of \(\rf\), we only need to check the requirement for \(\partial_{ji} \rf\).
	In other words, we need
	\[
		\max_{i,j}|\partial_{x_i x_j, y_i y_j}\C(t,t)
		+ \partial_{x_i x_j, y_i y_j}\C(s,s)
		-2\partial_{x_i x_j, y_i y_j}\C(s,t)| \le K |\ln(|t-s|)|^{-(1+\alpha)}
	\]
	But that is \eqref{eq: sufficient for continuous second derivative} in
	Assumption~\ref{assmpt: smoothness assumption} written in terms of
	covariances. By application
	of Corollary~\ref{cor: gaussian kac-rice}, we therefore get
	\begin{align*}
		\Lambda(\Vol, M) 
		&= \E[\crit^\alpha(\Vol, M)]
		= \E[\level(f, g: \Vol, M)]\\
		&= \int_\Vol \E\left[
			|\det \nabla f(t)| \ind_M(\rf(t), \nabla^2\rf(t)) \mid f(t) = 0
		\right] \density_{\nabla\rf(t)}(0)dt.
	\end{align*}
	
	
	The second inequality follows as \(\nabla\rf(t)\) is independent of
	\((\rf(t),\nabla^2\rf(t))\) due to Corollary~\ref{cor: uncorr leads to indep
	in gaussian case} which turns the conditional expectation into a normal
	expectation. Lastly replace all \(t\) with \(0\) due to stationarity.
\end{proof}
\begin{remark}
	As can be seen in Theorem~\ref{thm: general kac-rice} \parencite[Theorem
	11.2.1]{adlerRandomFieldsGeometry2007}, \(\rf\) does not necessarily
	need to be Gaussian. We just need it and its derivatives to satisfy a
	list of properties which are satisfied for Gaussian process with
	smooth enough covariance functions.
\end{remark}

Due to stationarity, we have completely dropped the dependence of \(\rf\) on
\(t\) in Theorem~\ref{thm: kac-rice formula} to define \(\lambda\). To reduce
notational clutter, we will from now on write
\[
	(\rf,H):= (Z(0), \nabla^2\rf(0))
\]

\begin{corollary}[Mark Distribution of Critical Points in a Stationary Random Field]
	Let \(p(z,h)\) be the joint density of \((\rf, H)\) and define
	\[
		\Omega(z,h) := c |det(h)| p(z,h)
	\]
	with \(c\in\real\) such that \(\int \Omega(z,h)dz dh = 1\).
	If we take a random critical point \((T,\rf(T),\nabla^2\rf(T))\sim \crit\),
	then the distribution over its marks is given by
	\[
		\Pr_{\rf(T),\nabla^2\rf(T)}(A) = \int_A \Omega(z,h) dz dh.
	\]
\end{corollary}
\begin{remark}
	Notice how this distribution of marks is different from
	\[
		\Pr((\rf(t),\nabla^2\rf(t))\in A \mid \nabla\rf(t) = 0)
		\overset{\text{stationary}}
		= \Pr((\rf,H)\in A)
		= \int_A p(z,h)dz dh.
	\]
	In particular the density of marks is \(\Omega\) not \(p\), like it is if
	we simply condition on the fact that \(t\) is a critical point.
\end{remark}
\begin{proof}
	With Lemma~\ref{lem: Campbell}, we have that the distribution of marks is
	given by
	\begin{align*}
		\Pr_{\rf(T),\nabla^2\rf(T)}(L)
		&= \frac{\lambda(L)}{\lambda(\real)}
		\overset{\ref{thm: kac-rice formula}}= \frac{
			\E[|\det(\nabla^2 \rf(0))|
			\ind_L(\rf(0), \nabla^2\rf(0))
			]
		}{
			\E[|\det(\nabla^2 \rf(0))|]
		}\\
		&= \int_L \Omega(z,h) dz dh.
		\qedhere
	\end{align*}
\end{proof}

\subsection{Height Distribution of Minima}

Of particular interest, is the height distribution of minima
\[\begin{aligned}
	\Pr(\rf(T)\in A \mid T\text{ is minimum})
	&=\Pr(\rf(T)\in A \mid \negEV[\nabla^2\rf(T)]=0)\\
	&= \frac{\Pr_{(\rf(T), \nabla^2\rf(T))}(A\times \negEV^{-1}(0))}{\Pr(\negEV[\nabla^2\rf(T)]=0)}\\
	&= \int_A \underbrace{
		\frac{\int_{\negEV^{-1}(0)} \Omega(z,h) dh}{\Pr(T\text{ is min})}
	}_{=: \phi(z)} dz.
\end{aligned}\]
It turns out to be absolutely continuous with density \(\phi\). Unfortunately
our term for \(\phi\) looks horrible. While \(\Omega\) itself might be
calculable, the set \(\negEV^{-1}(0)\) is anything but pretty. We are therefore
going to switch to the eigenvalues \(\lambda_1,\dots,\lambda_\dimension\) of
the random matrix \(H\). We have
\[
	\det(H) = \prod_{i=1}^\dimension \lambda_i
	\quad\text{and}\quad
	\ind_{\negEV(H) = 0} = \prod_{i=1}^\dimension \ind_{\lambda_i\ge 0}
\]
this leads to 
\[\begin{aligned}
	&\Pr_{(\rf(T), \nabla^2\rf(T))}(A\times \negEV^{-1}(0))\\
	&\overset{\text{def. }\Omega}= c\int_{A\times\negEV^{-1}(0)} |\det(h)| p(z,h)dz dh\\
	&\overset{\text{def. } p}= c\E[\ind_{A}(Z)\ind_{\negEV^{-1}(0)}(H)|\det(H)|]\\
	&= c\E[\ind_{A}(Z)\prod_{i=1}^\dimension \lambda_i \ind_{\lambda_i \ge 0}]\\
	&= \int_A c\underbrace{
		\int \prod_{i=1}^\dimension x_i \ind_{x_i\ge 0}
		\Pr((\lambda_1,\dots,\lambda_\dimension)\in dx\mid Z=z) \varphi_Z(z)
	}_{\sim\phi(z)}
	dz,
\end{aligned}\]
where \(\varphi_Z(z)\) is the marginal density of \(Z=Z(0)\). At first glance
the conditional distribution of the eigenvalues \(\lambda_i\) given the
height \(Z\) is just as difficult. But we will find out soon, that this
conditional distribution is more or less just the distribution of Eigenvalues of
the Gaussian Orthogonal Ensemble (GOE) shifted by \(Z\).
