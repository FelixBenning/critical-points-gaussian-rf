\section{Joint Distribution Derivation}


In this section we want to derive the joint density of \((\rf, H):=
(\rf(0), \nabla^2\rf(0))\). As \(\partial_{ij}\rf\) are limits of differences
of multivariate normal random variables, this is a centered multivariate normal
random vector. We therefore only need to consider the covariance. Additionally
we know \(H_{ij} = H_{ji}\), so we are only interested
in \((\rf, (H_{ij})_{i\le j})\).

As for the covariances, we have \fxnote{reference}
\begin{align*}
	\Cov(\rf,\rf) &= \sqC(0)\\
	\Cov(\rf, H_{ij}) &= \delta_{ij} \sqC'(0)\\
	\Cov(H_{ij}, H_{kl}) &= \sqC''(0)
	[\delta_{ij}\delta_{kl} + \delta_{ik}\delta_{jl} + \delta_{il}\delta_{kj}].
\end{align*}
In particular for \(i<j\), we have
\[
	\Cov(\rf, H_{ij}) = 0,
\]
and also
\[
	\Cov(H_{ij}, H_{kl}) = \sqC''(0)
	[\delta_{ik}\delta_{jl} + \underbrace{\delta_{il}\delta_{kj}}_{=0}],
\]
because \(\delta_{il}\delta_{kj}=1\) would imply \(i < j = k \le l = i\), which
is a contradiction. So this covariance is only non-zero, if \(i=k\) and \(j=l\),
i.e. \(H_{ij}\) for \(i<j\) has covariance zero with all other terms.
As we are in the multivariate Gaussian case, this also implies independence.
Shifting all these independent mixed derivatives to the end, our covariance
matrix looks like
\[
	\Sigma := \Cov(\rf, (H_{ii})_{i\le\dimension}, (H_{ij})_{i<j})
	= \begin{pmatrix}
		B & 0\\
		0 & \frac{\sqC''(0)}{\dimension}\identity
	\end{pmatrix}
\]
with
\[
	B = \begin{pmatrix}
		\sqC(0) & \sqC'(0)	& \cdots & \sqC'(0) \\
		\sqC'(0) & 3\sqC''(0) &  \cdots & \sqC''(0) \\
		\vdots & \vdots & \ddots & \vdots \\
		\sqC'(0) & \sqC''(0) & \cdots &  3 \sqC''(0)
	\end{pmatrix}
	% =: \begin{pmatrix}
	% 	d & b \\
	% 	b^T & \Sigma_\partial
	% \end{pmatrix}.
\]
As the distribution of centered multivariate normal random variables is fully
determined by their covariance matrix, we can manually construct
\(Z,(H_{ij})_{i\le j}\) with the correct distribution. But by the way of
construction we will obtain more information and tools.

Sample independently
\begin{enumerate}
	\item \(Z\sim\normal(0, \sqC(0))\)
	\item \(\epsilon \sim \normal(0, \sqC''(0) - \frac{\sqC'(0)^2}{\sqC(0)})\)
	``left-over shift''\footnote{
		Why is \(\sqC''(0) - \frac{\sqC'(0)^2}{\sqC(0)} \ge 0\)? Consider the
		vector \(v:=\left(-\sqC'(0), \frac{\sqC(0)}\dimension, \dots,
		\frac{\sqC(0)}\dimension\right)\).  Since \(B\) is a covariance matrix, it
		is necessarily positive definite.
		So we have
		\[
			0 \le v B v^T
			= \frac{\dimension+2}\dimension\sqC''(0)\sqC(0)^2 - \sqC'(0)^2\sqC(0)
		\]
		Dividing both sides by \(\sqC(0)^2 > 0\) and letting \(\dimension\to\infty\)
		leads to the required inequality. So for covariance models valid in all
		dimensions, we have this inequality.
	}
	\item \(X_{11},\dots,X_{\dimension\dimension}\sim\normal(0, 2)\) and
	independently \(X_{ji}:=X_{ij}\sim\normal(0,1)\) for \(i<j\le\dimension\).
	In other words: \(X = (X_{ij})\) is a Gaussian Orthogonal Ensemble (GOE).
\end{enumerate}
We now define
\[
	H := \underbrace{\sqrt{\sqC''(0)}X}_{=:\tilde{H}}
	+ \overbrace{\underbrace{[\epsilon + \tfrac{\sqC'(0)}{\sqC(0)} Z]}_{=:H_S}\identity}^{\text{``shift''}}
\]
By construction \((Z,H)\) is centered multivariate normal. It is an easy
exercise to check that the covariance structure is correct.

As \(X\) is independent of \(H_S\), its eigenvalues
\(\tilde{\lambda}_1,\dots,\tilde{\lambda}_\dimension\) are independent of
\(H_S\). Assume that
\[
	X = P\diag[\tilde{\lambda}_1,\dots,\tilde{\lambda}_\dimension]P^{-1}
\]
Since the identity matrix is commonly diagonizable with every matrix, this
results in
\[
	H = P(\sqrt{\sqC''(0)} \diag[\tilde{\lambda}_1,\dots,\tilde{\lambda}_\dimension] + H_S) P^{-1}
\]
in other words, the eigenvalues of \(H\) are
\[
	\lambda_i
	:= \sqrt{\sqC''(0)}\tilde{\lambda}_i + \underbrace{\epsilon + \tfrac{\sqC'(0)}{\sqC(0)}\rf}_{=H_S}.
\]
The eigenvalues of \(H\) are therefore distributed like the eigenvalues of the
GOE multiplied by a common factor and shifted by an independent shift \(H_S\)
consisting of the current height \(\rf\) and an additional random component
\(\epsilon\).

\subsection{\texorpdfstring{From \(\rf\) to \(\lambda_1,\dots,\lambda_\dimension\)}{From Z to 位1,...,位N}}

In particular we have
\[
	(\lambda_1,\dots,\lambda_\dimension \mid \rf = z)
	\overset{d}= 
	(\sqrt{\sqC''(0)}\tilde{\lambda}_i + \epsilon + \tfrac{\sqC'(0)}{\sqC(0)}z)_{i=1,\dots,\dimension}
\]
More specifically, we have
\[
	\begin{aligned}
		&\int f(x) \Pr(\lambda_1,\dots,\lambda_\dimension \in dx\mid \rf=z)\\
		&= \int
		\int f(x) \Pr(\lambda_1,\dots,\lambda_\dimension \in dx\mid \rf=z, \epsilon=t)
		\Pr(\epsilon\in dt)\\
		&= \int \int f\left(y + t + \tfrac{\sqC'(0)}{\sqC(0)}z\right)
		\Pr(\sqrt{\sqC''(0)}(\tilde{\lambda}_1,\dots,\tilde{\lambda}_\dimension)\in dy)
		\Pr(\epsilon \in dt)\\
		&= \int \int f\left(y + t + \tfrac{\sqC'(0)}{\sqC(0)}z\right)
		\varphi_{\text{GOE}}\left(\frac{y}{\sqrt{\sqC''(0)}}\right) dy \varphi_\epsilon(t)dt\\
		\overset{y_i=\sqrt{\sqC''(0)}x_i}&= \sqrt{\sqC''(0)}^\dimension \int \int f\left(\sqrt{\sqC''(0)}x + t + \tfrac{\sqC'(0)}{\sqC(0)}z\right)
		\varphi_{\text{GOE}}(x) dx \varphi_\epsilon(t)dt
	\end{aligned}
\]
where \(\varphi_{\text{GOE}}\) is the joint distribution of the eigenvalues of
the GOE and \(\varphi_\epsilon\) is the density of \(\normal\left(0,
\sqC''(0)-\tfrac{\sqC'(0)^2}{\sqC(0)}\right)\).

\subsection{\texorpdfstring{From \(\lambda_1,\dots,\lambda_\dimension\) to \(\rf\)}{From 位1,...,位N to Z}}

In this section we want to determine the distribution of \(\rf\mid
\lambda_1,\dots,\lambda_\dimension\). As a warmup we will first consider
\(\E[Z\mid H_S]\). \(H_S\) is what we can obtain from \(\lambda_i\) by averaging
if we have sufficiently many. But we can not really separate \(\epsilon\) from
\(\rf\) using only \(\lambda_i\), so \(\rf\mid H_S\) will end up being
similar to \(\rf\mid\lambda_1,\dots,\lambda_\dimension\) for large \(\dimension\).

\subsubsection{\texorpdfstring{\(\rf\mid H_S\)}{Z|H0}}

Since \(\rf\) and \(H_S\) are multivariate normal, we have 
\[
	\rf\mid H_S\sim \normal(\E[\rf\mid H_S], \Var(\rf\mid H_S)).
\]
So we only need to calculate the conditional expectation \(\E[\rf\mid H_S]\) and
the remaining variance
\[
	\Var(\rf\mid H_S)\overset{\text{def}}
	=\E[(\rf-\E[\rf\mid H_S])^2\mid H_S].
\]
For the conditional expectation it is sufficient to calculate the best linear
unbiased estimator (BLUE) as it coincides with the conditional expectation in
the Gaussian case.

Let \(\langle X, Y\rangle := \E[XY]\) and \(\|\cdot\|^2\) the induced norm.
Then we want to find the \(\alpha\in\real\) which minimizes
\[
	\|Z-\alpha H_S\|^2
	= (1-\tfrac{\sqC'(0)}{\sqC(0)}\alpha)^2 \underbrace{\|\rf\|^2}_{=\sqC(0)}
	+ \alpha^2 \underbrace{\|\epsilon\|^2}_{=\sqC''(0)\mathrlap{-\frac{\sqC'(0)^2}{\sqC(0)}}}
\]
The first order condition is therefore
\[
	0\overset!= -2 \tfrac{\sqC'(0)}{\cancel{\sqC(0)}}
	(1-\bcancel{\tfrac{\sqC'(0)}{\sqC(0)}\alpha})\cancel{{\sqC(0)}} + 2\alpha
	\left(\sqC''(0)-\bcancel{\tfrac{\sqC'(0)^2}{\sqC(0)}}\right),
\]
which leads to \(\alpha = \frac{\sqC'(0)}{\sqC''(0)}\) and therefore
\[
	\E[\rf\mid H_S] = \tfrac{\sqC'(0)}{\sqC''(0)} H_S.
\]
Using Lemma~\ref{lem: conditional variance in the gaussian case} we finally
have
\[\begin{aligned}
	\Var(\rf\mid H_S)
	\overset{\ref{lem: conditional variance in the gaussian case}}&=
	\Var(\rf) - \Var(\E[\rf\mid H_S])
	= \sqC(0) - \tfrac{\sqC'(0)^2}{\sqC''(0)^2}\underbrace{\Var(H_S)}_{
		= \Var(\epsilon) \mathrlap{+ \frac{\sqC'(0)^2}{\sqC(0)^2}\Var(\rf)}
	}\\
	&= \sqC(0) - \tfrac{\sqC'(0)^2}{\sqC''(0)^2}\left[
		\sqC''(0) - \cancel{\tfrac{\sqC'(0)^2}{\sqC(0)} +  \tfrac{\sqC'(0)^2}{\sqC(0)}}
	\right]\\
	&=\sqC(0) - \tfrac{\sqC'(0)^2}{\sqC''(0)}.
\end{aligned}\]
In summary we have:
\[
	\rf\mid H_S \sim \normal\left(
		\tfrac{\sqC'(0)}{\sqC''(0)} H_S,
		\sqC(0) - \tfrac{\sqC'(0)^2}{\sqC''(0)}
	\right)
\]

\subsubsection{\texorpdfstring{\(\rf\mid H_1,\dots,H_\dimension\)}{Z|H1,...,HN}}

As another intermediate step, we are going to calculate the distribution of
\(\rf\mid H_1,\dots,H_\dimension\) using the diagonal elements
\(H_1,\dots,H_\dimension\) of \(H\). We will later find out that it is identical
to the one we want anyway.

Using the same approach via the BLUE, we want to minimize
\[
	\begin{aligned}
	&\|\rf - \sum_{i=1}^\dimension\alpha_i H_i \|^2\\
	&= \left(1- \frac{\sqC'(0)}{\sqC(0)}\sum_{i=1}^\dimension \alpha_i\right)^2
	\underbrace{\|\rf\|^2}_{\sqC(0)}
	+  \left(\sum_{i=1}^\dimension \alpha_i\right)^2 \underbrace{\|\epsilon\|^2}_{
		=\sqC''(0)-\tfrac{\sqC'(0)^2}{\sqC(0)}
	}
	+ \sum_{i=1}^\dimension \alpha_i^2 \underbrace{\|\tilde{H}_i\|^2}_{2\sqC''(0)}.
	\end{aligned}
\]
We are going to minimize this term in two steps.
\begin{enumerate}
	\item Fix \(\sum_{i=1}^\dimension \alpha_i = s\) and minimize over
	\(\alpha_i\) with this constraint,
	\item minimize the result over \(s\).
\end{enumerate}
In the first part, everything is constant except for the last part. So
we need to find the \(\alpha_i\) which minimize
\[
	\sum_{i=1}^\dimension \alpha_i^2
\]
under the given constraint \(\sum_{i=1}^\dimension \alpha_i = s\). As it turns
out
\[
	1 = \left(\sum_{i=1}^\dimension \frac{\alpha_i}s\right)^2
	= \left(\frac1\dimension\sum_{i=1}^\dimension \frac\dimension{s}\alpha_i\right)^2
	\overset{(\cdot)^2 \text{ convex.}}\le \frac1\dimension \sum_{i=1}^\dimension 
	\left(\frac\dimension{s}\alpha_i\right)^2
	= \frac\dimension{s^2} \sum_{i=1}^\dimension \alpha_i,
\]
implies
\[
	\sum_{i=1}^\dimension \left(\frac{s}\dimension\right)^2 = \frac{s^2}\dimension
	\le \sum_{i=1}^\dimension \alpha_i.
\]
So \(\alpha_i=\frac{s}\dimension\) minimizes our constraint optimization problem.
Inserting this solution into our problem results in
\[
	\left(1- \tfrac{\sqC'(0)}{\sqC(0)}s\right)^2
	\sqC(0)
	+  s^2 \left(
		\sqC''(0)-\tfrac{\sqC'(0)^2}{\sqC(0)}
	\right)
	+ 2\frac{s^2}\dimension \sqC''(0).
\]
Except for the last summand this is the same optimization problem as before.
The first order condition results in
\[
	s = \frac{\sqC'(0)}{\sqC''(0)(1+\tfrac4{\dimension})}
\]
So we finally get
\begin{align*}
	\E[\rf\mid H_1,\dots,H_\dimension]
	&= \frac{\sqC'(0)}{\sqC''(0)(1+\tfrac4{\dimension})}
	\frac1\dimension\underbrace{\sum_{i=1}^\dimension H_i}_{
		=\trace(H) \mathrlap{=\sum_{i=1}^\dimension\lambda_i}
	}\\
	&= \frac{\sqC'(0)}{\sqC''(0)(1+\tfrac4{\dimension})}
	\bar{\lambda}
\end{align*}
where \(\bar{\lambda}\) is the mean eigenvalue. Lastly we want to determine
the explained variance
\begin{align*}
	\Var(\E[\rf\mid H_1,\dots,H_\dimension])
	&= \left(\frac{\sqC'(0)}{\sqC''(0)(1+\tfrac4{\dimension})}\right)^2
	\Var\left(
		\epsilon + \tfrac{\sqC'(0)}{\sqC(0)}\rf + \frac{1}{\dimension}\sum_{i=1}^\dimension \tilde{H_i}
	\right)\\
	&= \frac{\sqC'(0)^2}{\sqC''(0)^2(1+\tfrac4{\dimension})^2}
	\left(
		\Var(\epsilon) + \tfrac{\sqC'(0)^2}{\sqC(0)} + \tfrac1\dimension\Var(\tilde{H}_1)
	\right)\\
	&= \frac{\sqC'(0)^2}{\sqC''(0)^2(1+\tfrac4{\dimension})^2}
	(\sqC''(0) + \tfrac2\dimension\sqC''(0))\\
	&= \frac{\sqC'(0)^2(1+\tfrac2\dimension)}{\sqC''(0)(1+\tfrac4{\dimension})^2}.
\end{align*}
Since the off-diagonal elements are independent from \(\rf\), we get with
Lemma~\ref{lem: conditional variance in the gaussian case}
\[
	\rf\mid H = \rf \mid H_1,\dots, H_\dimension \sim \normal\left(
		\frac{\sqC'(0)}{\sqC''(0)(1+\tfrac4{\dimension})}\bar{\lambda},
		\sqC(0)-\frac{\sqC'(0)^2(1+\tfrac2\dimension)}{\sqC''(0)(1+\tfrac4{\dimension})^2}
	\right).
\]
So we know that \(\E[f(\rf)\mid H]\) is \(\lambda_1,\dots,\lambda_\dimension\)
measurable. And since \(\lambda_1,\dots,\lambda_\dimension\) are as eigenvalues
of \(H\) \(\sigma(H)\)-measurable, we have \(\sigma(H)\supseteq \sigma(\lambda_1,\dots,\lambda_\dimension)\)
and therefore get
\[
	\E[f(Z)\mid \lambda_1,\dots,\lambda_\dimension]
	= \E[\E[f(Z)\mid H] \mid \lambda_1,\dots,\lambda_\dimension]
	= \E[f(Z)\mid H].
\]
Which finally implies \(\rf\mid H = \rf\mid \lambda,\dots,\lambda_\dimension\).


\subsection{Height Distribution}

The height distribution
of minima is therefore up to constants
\[\begin{aligned}
	&\phi(z)\\
	&\sim\int \prod_{i=1}^\dimension x_i \ind_{x_i\ge 0}
	\Pr((\lambda_1,\dots,\lambda_\dimension)\in dx\mid Z=z) \varphi_Z(z)\\
	&\sim\int \int \prod_{i=1}^\dimension
	\left(\sqrt{\sqC''(0)}x_i + t + \tfrac{\sqC'(0)}{\sqC(0)}z\right)
	\ind_{x_i\ge - \tfrac{t+\frac{\sqC'(0)}{\sqC(0)}z}{\sqrt{\sqC''(0)}}}
	\varphi_{\text{GOE}}(x) dx \varphi_\epsilon(t)dt \varphi_Z(z)\\
\end{aligned}\]
Let us first have a look at
\[
	\int\prod_{i=1}^\dimension (c x_i + b) \ind_{x_i \ge -\tfrac{b}{c}}\varphi_{\text{GOE}}(x)dx
\]

