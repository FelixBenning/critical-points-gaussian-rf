\section{Joint Distribution Derivation}


In this section we want to derive the joint density of \((\rf, H):=
(\rf(0), \nabla^2\rf(0))\), since we need it to calculate
\begin{enumerate}
	\item the intensity \(\intensity = \E[|\det(H)|]\varphi_\rf(0)\),
	\item the relative intensity \(\Pr(\negEV(H_T) = \alpha) =
	\frac{\E[|\det(H)|\ind_{\negEV(H)=\alpha}]}{\E[|\det(H)|]} \),
	\item and the conditional mark distribution
	\[
		\Pr(\Mark\in L\mid \negEV(H_T) = \alpha)
		= \frac{
			\E[|\det(H)|\ind_{\negEV(H) = \alpha}\ind_{(Z,H)\in L}]
		}{\E[|\det(H)|\ind_{\negEV(H) = \alpha}]}.
	\]
\end{enumerate}
As \(\partial_{ij}\rf\) are limits of differences of multivariate
normal random variables, this is a centered multivariate normal
random vector. We therefore only need to consider the covariance. Additionally
we know \(H_{ij} = H_{ji}\), so we are only interested
in \((\rf, (H_{ij})_{i\le j})\).

As for the covariances, we have by repeated application of Lemma~\ref{lem:
covariance of derivative}
\begin{align*}
	\Cov(\rf,\rf) &= \sqC(0)\\
	\Cov(\rf, H_{ij}) &= \delta_{ij} \sqC'(0)\\
	\Cov(H_{ij}, H_{kl}) &= \sqC''(0)
	[\delta_{ij}\delta_{kl} + \delta_{ik}\delta_{jl} + \delta_{il}\delta_{kj}].
\end{align*}
In particular for \(i<j\), we have
\[
	\Cov(\rf, H_{ij}) = 0,
\]
and also
\[
	\Cov(H_{ij}, H_{kl}) = \sqC''(0)
	[\delta_{ik}\delta_{jl} + \underbrace{\delta_{il}\delta_{kj}}_{=0}],
\]
because \(\delta_{il}\delta_{kj}=1\) would imply \(i < j = k \le l = i\), which
is a contradiction. So this covariance is only non-zero, if \(i=k\) and \(j=l\),
i.e. \(H_{ij}\) for \(i<j\) has covariance zero with all other terms.
As we are in the multivariate Gaussian case, this also implies independence.
Shifting all these independent mixed derivatives to the end, our covariance
matrix looks like
\[
	\Sigma := \Cov(\rf, (H_{ii})_{i\le\dims}, (H_{ij})_{i<j})
	= \begin{pmatrix}
		B & 0\\
		0 & \frac{\sqC''(0)}{\dims}\identity
	\end{pmatrix}
\]
with
\[
	B = \begin{pmatrix}
		\sqC(0) & \sqC'(0)	& \cdots & \sqC'(0) \\
		\sqC'(0) & 3\sqC''(0) &  \cdots & \sqC''(0) \\
		\vdots & \vdots & \ddots & \vdots \\
		\sqC'(0) & \sqC''(0) & \cdots &  3 \sqC''(0)
	\end{pmatrix}
	% =: \begin{pmatrix}
	% 	d & b \\
	% 	b^T & \Sigma_\partial
	% \end{pmatrix}.
\]
As the distribution of centered multivariate normal random variables is fully
determined by their covariance matrix, we can manually construct
\(Z,(H_{ij})_{i\le j}\) with the correct distribution. But by the way of
construction we will obtain more information and tools.

Sample independently
\begin{enumerate}
	\item \(Z\sim\normal(0, \sqC(0))\)
	\item \(\epsilon \sim \normal(0, \sqC''(0) - \frac{\sqC'(0)^2}{\sqC(0)})\)
	``left-over shift''\footnote{
		Why is \(\sqC''(0) - \frac{\sqC'(0)^2}{\sqC(0)} \ge 0\)? Consider the
		vector \(v:=\left(-\sqC'(0), \frac{\sqC(0)}\dims, \dots,
		\frac{\sqC(0)}\dims\right)\).  Since \(B\) is a covariance matrix, it
		is necessarily positive definite.
		So we have
		\[
			0 \le v B v^T
			= \frac{\dims+2}\dims\sqC''(0)\sqC(0)^2 - \sqC'(0)^2\sqC(0)
		\]
		Dividing both sides by \(\sqC(0)^2 > 0\) and letting \(\dims\to\infty\)
		leads to the required inequality. So for covariance models valid in all
		dimensions, we have this inequality.
	}
	\item \(\tilde{H}_{11},\dots,\tilde{H}_{\dims\dims}\sim\normal(0,
	a)\) and independently
	\(\tilde{H}_{ji}:=\tilde{H}_{ij}\sim\normal(0,\frac{a}{2})\) for
	\(i<j\le\dims\). And we choose \(a=2\sqC''(0)\).
	In other words: \(\tilde{H} = (X_{ij})\) is a (scaled) Gaussian Orthogonal
	Ensemble (GOE)\footnote{
		Oftentimes the definition of the GOE demands \(a=2\). To make notation
		easier, we follow \textcite{fyodorovHighDimensionalRandomFields2013} in
		this this more general definition.
		For real symmetric matrices \(h\) the density of the GOE is then given by
		\[
			\varphi^{\GOE}_{\dims,a}(h)
			= c_{\dims,a}\exp\left(-\frac{\dims}{2a}\trace(h^2)\right)
		\]
		with scaling constant
		\[
			(c^{\GOE}_{\dims,a})^{-1}
			= (\pi a)^{-\frac{\dims(\dims+1)}{4}}2^{-\dims/2}.
		\]
		The joint density of eigenvalues
		\(\evGOE=(\evGOE_1,\dots,\evGOE_\dims)\) is known to be
		\begin{equation}\label{eq: GOE ev density}
			\varphi^{\evGOE}_{\dims, a}(x_1,\dots,x_n)
			= c^\evGOE_{\dims,a}
			\exp\Bigl(-\frac{1}{2a}\sum_{i=1}^\dims x_i^2\Bigr)
			\underbrace{\prod_{i<j}|x_i - x_j|}_{=:\Delta_\dims}
		\end{equation}
		where \(\Delta_\dims\) is the so-called Vandermonde Determinant and
		the scaling constant is given by
		\[
			(c^\evGOE_{\dims,a})^{-1}
			= (2\pi)^{\frac{\dims}2}a^{\frac{\dims(\dims+1)}4}
			\prod_{j=1}^\dims \frac{\Gamma(1+\frac{j}2)}{\Gamma(\frac32)}.
		\]
	}.
\end{enumerate}
We now define
\[
	H := \tilde{H}
	+ \overbrace{\underbrace{[\epsilon + \tfrac{\sqC'(0)}{\sqC(0)} Z]}_{=:\shift}\identity}^{\text{``shift''}}
\]
By construction \((Z,H)\) is centered multivariate normal. It is an easy
exercise to check that the covariance structure is correct.

As \(\tilde{H}\) is independent of \(\shift\), its eigenvalues
\(\evGOE=(\evGOE_1,\dots,\evGOE_\dims)\) are independent of
\(\shift\). Assume that
\[
	X = P\diag[\evGOE]P^{-1}
\]
Since the identity matrix is commonly diagonizable with every matrix, this
results in
\[
	H = P(\diag[\evGOE] + \shift\identity) P^{-1}
\]
in other words, the eigenvalues of \(H\) are
\[
	\evH_i
	:= \evGOE_i + \underbrace{\shift}_{=\epsilon \mathrlap{+ \tfrac{\sqC'(0)}{\sqC(0)}\rf}}.
\]
The eigenvalues of \(H\) are therefore distributed like the eigenvalues of the
GOE multiplied by a common factor and shifted by an independent shift \(\shift\)
consisting of the current height \(\rf\) and an additional random component
\(\epsilon\).

\subsection{\texorpdfstring{\(\evH\mid\rf\)}{Λ|Z}}
\label{subsec: Lambda|rf}

In particular we have for \(\evH = (\evH_1,\dots,\evH_\dims)\)
\[
	(\evH \mid \rf = z)
	\overset{d}= 
	\evGOE \dotPlus (\epsilon + \tfrac{\sqC'(0)}{\sqC(0)}z),
\]
where \(\dotPlus\) is entry-wise addition.
More specifically we have
\begin{equation}\label{eq: expectation f(Lambda) | Z}
		\E[f(\evH)\mid Z=z]
		= \E\left[
			f\left(\evGOE \dotPlus (\epsilon+\tfrac{\sqC'(0)}{\sqC(0)}z)\right)
		\right].
\end{equation}

\subsection{\texorpdfstring{\(\rf\mid\evH\)}{Z|Λ}}

\begin{theorem}[Conditional Distributions of \(\rf\)]
	\label{thm: conditional distributions of rf}
	We have
	\begin{align}
		\rf\mid \shift
		&\sim \normal\left(
			\tfrac{\sqC'(0)}{\sqC''(0)} \shift,
			\sqC(0) - \tfrac{\sqC'(0)^2}{\sqC''(0)}
		\right)\\
		\rf\mid \evH
		&=\rf\mid H = \rf \mid H_1,\dots, H_\dims
		\nonumber\\
		&\sim \normal\left(
		\frac{\sqC'(0)}{\sqC''(0)(1+\tfrac4{\dims})}\bar{\evH},
		\sqC(0)-\frac{\sqC'(0)^2(1+\tfrac2\dims)}{\sqC''(0)(1+\tfrac4{\dims})^2}
	\right)
	\end{align}
	where the mean of eigenvalues (or the mean of the diagonal)
	\[
		\bar{\evH}
		:=\frac1\dims\sum_{i=1}^\dims \evH_i
		=\frac{\trace{H}}\dims
	\]
	approximate the shift \(\shift\) and therefore approximates \(\rf\mid \shift\) in
	the limit.
\end{theorem}
\begin{proof}
	See Appendix~\ref{sec: proof of conditionals}.
\end{proof}
\begin{remark}
	The conditional variance \(\Var(\rf\mid \shift)= \sqC(0) -
	\tfrac{\sqC'(0)^2}{\sqC''(0)}\) is zero, iff \(\epsilon\) is almost surely
	zero (because it has the same variance up to a factor). In that case the
	shift \(\shift\) consists only of \(\rf\) so this should not be surprising.
\end{remark}


